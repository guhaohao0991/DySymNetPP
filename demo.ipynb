{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grep: warning: GREP_OPTIONS is deprecated; please use an alias or script\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use Device: Place(cpu)\n",
      "************************* Start Sampling... *************************\n",
      "\n",
      "******************** Epoch 00 ********************\n",
      "Operators of each layer obtained by sampling:  {1: ['pow2', '+', '+', '+', 'pow3', '*'], 2: ['+', 'pow2', '+', 'pow3', '*', 'pow3'], 3: ['pow2', '*', 'pow3', 'id', '+', 'pow2']}\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 4.69223627546263e+21\tTest mse: 1.5250230880854108e+35\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 1.2239528408796879e+21\tTest mse: 4.53339683533698e+34\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 3.749339323640578e+18\tTest mse: 1.1054557262621831e+26\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 2355757081165824.0\tTest mse: 4.685378238986066e+23\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 3.5345663847556973e+19\tTest mse: 8.635533746013756e+28\n",
      "Final expression:  None\n",
      "Test R2:  0.0\n",
      "Test error:  10000\n",
      "Relative error:  100\n",
      "Reward:  9.999000099990002e-05\n",
      "\n",
      "\n",
      "Operators of each layer obtained by sampling:  {1: ['+', 'pow2'], 2: ['pow2', 'pow2'], 3: ['+', 'pow3']}\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 112366216.0\tTest mse: 5791660113920.0\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 107515792.0\tTest mse: 555267364225024.0\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 275950764032.0\tTest mse: 8.702167241405235e+16\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 146750320.0\tTest mse: 284423880704.0\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 966677.25\tTest mse: 1532679168.0\n",
      "Final expression:  None\n",
      "Test R2:  0.0\n",
      "Test error:  10000\n",
      "Relative error:  100\n",
      "Reward:  9.999000099990002e-05\n",
      "\n",
      "\n",
      "Operators of each layer obtained by sampling:  {1: ['pow2', 'pow2', 'id', 'id'], 2: ['pow2', 'pow3', 'pow3', '*'], 3: ['pow3', 'pow2', 'pow2', 'pow2'], 4: ['*', 'pow3', '+', 'id'], 5: ['*', 'pow3', 'pow2', 'pow3']}\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/shared/DySymNetPP/DySymNetPP/SymbolicRegression.py:27: SymPyDeprecationWarning: \n",
      "\n",
      "Passing the function arguments to lambdify() as a set is deprecated. This\n",
      "leads to unpredictable results since sets are unordered. Instead, use a list\n",
      "or tuple for the function arguments.\n",
      "\n",
      "See https://docs.sympy.org/latest/explanation/active-deprecations.html#deprecated-lambdify-arguments-set\n",
      "for details.\n",
      "\n",
      "This has been deprecated since SymPy version 1.6.3. It\n",
      "will be removed in a future version of SymPy.\n",
      "\n",
      "  sp_expr = sp.lambdify(free_symbols, func)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Final expression:  None\n",
      "Test R2:  0.0\n",
      "Test error:  10000\n",
      "Relative error:  100\n",
      "Reward:  9.999000099990002e-05\n",
      "\n",
      "\n",
      "Operators of each layer obtained by sampling:  {1: ['id', '+', '*', 'pow3', 'id', '+'], 2: ['pow2', 'pow2', 'id', '*', '+', '*'], 3: ['pow2', 'pow2', '*', 'pow2', '*', '*'], 4: ['+', 'pow2', '+', 'pow3', 'id', 'pow2'], 5: ['pow3', '*', 'pow3', 'id', 'pow3', 'id']}\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Final expression:  None\n",
      "Test R2:  0.0\n",
      "Test error:  10000\n",
      "Relative error:  100\n",
      "Reward:  9.999000099990002e-05\n",
      "\n",
      "\n",
      "Operators of each layer obtained by sampling:  {1: ['+', '+', 'id', 'pow2'], 2: ['pow2', 'pow2', 'pow2', 'id'], 3: ['pow2', 'pow3', 'id', 'pow2'], 4: ['id', 'pow3', 'pow2', '+']}\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 2.0618216591526461e+18\tTest mse: 2.8314010780503604e+28\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 15360.1416015625\tTest mse: 6867838.0\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 5.2439474509778424e+23\tTest mse: inf\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 1.059641268543162e+34\tTest mse: inf\n",
      "Final expression:  None\n",
      "Test R2:  0.0\n",
      "Test error:  10000\n",
      "Relative error:  100\n",
      "Reward:  9.999000099990002e-05\n",
      "\n",
      "\n",
      "Operators of each layer obtained by sampling:  {1: ['+', 'id'], 2: ['pow2', '+'], 3: ['pow3', 'pow3'], 4: ['id', 'id'], 5: ['id', '+']}\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 1205777536.0\tTest mse: 363186847744.0\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 1.0436670780181885\tTest mse: 1.9595619440078735\n",
      "Epoch: 1000\tTotal training loss: 0.15871548652648926\tTest mse: 32.762149810791016\n",
      "Epoch: 2000\tTotal training loss: 0.15466152131557465\tTest mse: 34.510597229003906\n",
      "Epoch: 3000\tTotal training loss: 0.15249508619308472\tTest mse: 65.58064270019531\n",
      "Epoch: 4000\tTotal training loss: 0.15016108751296997\tTest mse: 65.58940124511719\n",
      "Epoch: 5000\tTotal training loss: 0.14558739960193634\tTest mse: 65.74479675292969\n",
      "Epoch: 6000\tTotal training loss: 0.13985119760036469\tTest mse: 65.51069641113281\n",
      "Epoch: 7000\tTotal training loss: 0.13942667841911316\tTest mse: 65.6623306274414\n",
      "Epoch: 8000\tTotal training loss: 0.13918524980545044\tTest mse: 65.8878173828125\n",
      "Epoch: 9000\tTotal training loss: 0.13901594281196594\tTest mse: 66.21111297607422\n",
      "Epoch: 10000\tTotal training loss: 0.13887789845466614\tTest mse: 66.65505981445312\n",
      "Epoch: 11000\tTotal training loss: 0.13875824213027954\tTest mse: 67.19325256347656\n",
      "Epoch: 12000\tTotal training loss: 0.13865014910697937\tTest mse: 67.83436584472656\n",
      "Epoch: 0\tTotal training loss: 0.13873818516731262\tTest mse: 67.4010238647461\n",
      "Epoch: 1000\tTotal training loss: 0.13854077458381653\tTest mse: 65.03437042236328\n",
      "Epoch: 2000\tTotal training loss: 0.13849042356014252\tTest mse: 65.04389953613281\n",
      "Epoch: 3000\tTotal training loss: 0.13844360411167145\tTest mse: 65.0534439086914\n",
      "Epoch: 4000\tTotal training loss: 0.13839977979660034\tTest mse: 65.06226348876953\n",
      "Epoch: 5000\tTotal training loss: 0.13835857808589935\tTest mse: 65.0714340209961\n",
      "Epoch: 6000\tTotal training loss: 0.13831967115402222\tTest mse: 65.0801773071289\n",
      "Epoch: 7000\tTotal training loss: 0.13828274607658386\tTest mse: 65.08820343017578\n",
      "Epoch: 8000\tTotal training loss: 0.13824766874313354\tTest mse: 65.0962142944336\n",
      "Epoch: 9000\tTotal training loss: 0.13821423053741455\tTest mse: 65.10302734375\n",
      "Epoch: 10000\tTotal training loss: 0.13818234205245972\tTest mse: 65.10960388183594\n",
      "Epoch1 time per sample: 8.76787747654177e-06 seconds/sample\n",
      "Epoch2 : 1.0411344138565881e-05 seconds/sample. N_sampel: 500\n",
      "error_expr_sorted [(65.10960388183594, -2.612680673599243, 2.12808622062071*(0.204766387354988*x_1**2 + x_1)**3)]\n",
      "Constructing BFGS loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0126 06:41:30.573946 20730 dygraph_functions.cc:81971] got different data type, run type promotion automatically, this may cause data type been changed.\n",
      "W0126 06:41:30.574728 20730 dygraph_functions.cc:87724] got different data type, run type promotion automatically, this may cause data type been changed.\n",
      "W0126 06:41:30.574885 20730 dygraph_functions.cc:83253] got different data type, run type promotion automatically, this may cause data type been changed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final expression:  2.19135148934435*(0.204928122638117*x_1**2 + x_1)**3\n",
      "Test R2:  0.9009802397543727\n",
      "Test error:  0.09173589714213912\n",
      "Relative error:  0.5104685352808921\n",
      "Reward:  0.9159724459163813\n",
      "\n",
      "\n",
      "Operators of each layer obtained by sampling:  {1: ['pow3', 'pow3', 'pow2'], 2: ['pow2', 'pow2', 'pow3']}\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 154.76885986328125\tTest mse: 416238.9375\n",
      "Epoch: 1000\tTotal training loss: 0.20879903435707092\tTest mse: 1054.139404296875\n",
      "Epoch: 2000\tTotal training loss: 0.2096867859363556\tTest mse: 1038.601318359375\n",
      "Epoch: 3000\tTotal training loss: 0.20675112307071686\tTest mse: 1616.550537109375\n",
      "Epoch: 4000\tTotal training loss: 0.2061055302619934\tTest mse: 1618.9326171875\n",
      "Epoch: 5000\tTotal training loss: 0.20561984181404114\tTest mse: 1630.2200927734375\n",
      "Epoch: 6000\tTotal training loss: 0.2053007185459137\tTest mse: 1617.5792236328125\n",
      "Epoch: 7000\tTotal training loss: 0.20496782660484314\tTest mse: 1628.1495361328125\n",
      "Epoch: 8000\tTotal training loss: 0.20467980206012726\tTest mse: 1634.4444580078125\n",
      "Epoch: 9000\tTotal training loss: 0.20446039736270905\tTest mse: 1634.04931640625\n",
      "Epoch: 10000\tTotal training loss: 0.2042676955461502\tTest mse: 1634.6162109375\n",
      "Epoch: 11000\tTotal training loss: 0.20409934222698212\tTest mse: 1636.2218017578125\n",
      "Epoch: 12000\tTotal training loss: 0.20395046472549438\tTest mse: 1637.5875244140625\n",
      "Epoch: 0\tTotal training loss: 0.20455783605575562\tTest mse: 1629.1898193359375\n",
      "Epoch: 1000\tTotal training loss: 0.20381996035575867\tTest mse: 1572.4866943359375\n",
      "Epoch: 2000\tTotal training loss: 0.20370809733867645\tTest mse: 1572.2901611328125\n",
      "Epoch: 3000\tTotal training loss: 0.2036028504371643\tTest mse: 1572.076904296875\n",
      "Epoch: 4000\tTotal training loss: 0.20350372791290283\tTest mse: 1571.87841796875\n",
      "Epoch: 5000\tTotal training loss: 0.20341016352176666\tTest mse: 1571.69921875\n",
      "Epoch: 6000\tTotal training loss: 0.20332184433937073\tTest mse: 1571.5191650390625\n",
      "Epoch: 7000\tTotal training loss: 0.20323829352855682\tTest mse: 1571.3511962890625\n",
      "Epoch: 8000\tTotal training loss: 0.20315919816493988\tTest mse: 1571.196044921875\n",
      "Epoch: 9000\tTotal training loss: 0.20308417081832886\tTest mse: 1571.043212890625\n",
      "Epoch: 10000\tTotal training loss: 0.20301291346549988\tTest mse: 1570.885986328125\n",
      "Epoch1 time per sample: 5.555217300451911e-06 seconds/sample\n",
      "Epoch2 : 6.073276062820354e-06 seconds/sample. N_sampel: 500\n",
      "error_expr_sorted [(1570.885986328125, -86.16239166259766, -3.70464*x_1**6 + 3.94745*(0.36409*x_1**3 + x_1**2)**2)]\n",
      "Constructing BFGS loss...\n",
      "Final expression:  -3.76401736901296*x_1**6 + 3.99858205121084*(0.36004392249736*x_1**3 + x_1**2)**2\n",
      "Test R2:  0.8188723757874982\n",
      "Test error:  0.16780393189340057\n",
      "Relative error:  0.8438640539135067\n",
      "Reward:  0.8563081290355528\n",
      "\n",
      "\n",
      "Operators of each layer obtained by sampling:  {1: ['+', 'id', '*'], 2: ['pow2', 'pow2', '*'], 3: ['+', '*', 'pow2']}\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 1030861.25\tTest mse: 55265180.0\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 1.0137295860027884e+18\tTest mse: 3.410483899195297e+22\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 2066327.25\tTest mse: 1173441024.0\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 5187225600.0\tTest mse: 1091220537344.0\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 760977272340480.0\tTest mse: 1.951480966893116e+19\n",
      "Final expression:  None\n",
      "Test R2:  0.0\n",
      "Test error:  10000\n",
      "Relative error:  100\n",
      "Reward:  9.999000099990002e-05\n",
      "\n",
      "\n",
      "Operators of each layer obtained by sampling:  {1: ['*', '*', '*'], 2: ['*', 'pow2', 'pow2'], 3: ['*', 'pow3', 'id'], 4: ['pow2', '*', '+']}\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 6.518506178417097e+27\tTest mse: inf\n",
      "Final expression:  None\n",
      "Test R2:  0.0\n",
      "Test error:  10000\n",
      "Relative error:  100\n",
      "Reward:  9.999000099990002e-05\n",
      "\n",
      "\n",
      "Operators of each layer obtained by sampling:  {1: ['pow2', 'pow2', '+', '*', '*', 'id'], 2: ['id', 'pow2', '*', 'id', 'pow3', 'pow3'], 3: ['pow3', '+', 'id', 'id', 'pow3', 'pow2'], 4: ['+', 'id', 'id', '+', '*', 'pow2']}\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Final expression:  None\n",
      "Test R2:  0.0\n",
      "Test error:  10000\n",
      "Relative error:  100\n",
      "Reward:  9.999000099990002e-05\n",
      "\n",
      "\n",
      "******************** Epoch 01 ********************\n",
      "Operators of each layer obtained by sampling:  {1: ['pow2', 'pow2'], 2: ['pow2', '*']}\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 29.29589080810547\tTest mse: 4184.798828125\n",
      "Epoch: 1000\tTotal training loss: 0.8521687984466553\tTest mse: 52.54685592651367\n",
      "Epoch: 2000\tTotal training loss: 0.8519739508628845\tTest mse: 52.89311218261719\n",
      "Epoch: 3000\tTotal training loss: 0.8503045439720154\tTest mse: 52.40993118286133\n",
      "Epoch: 4000\tTotal training loss: 0.8502845168113708\tTest mse: 52.5344123840332\n",
      "Epoch: 5000\tTotal training loss: 0.8502736687660217\tTest mse: 52.57086944580078\n",
      "Epoch: 6000\tTotal training loss: 0.8502684831619263\tTest mse: 52.580448150634766\n",
      "Epoch: 7000\tTotal training loss: 0.8502659201622009\tTest mse: 52.590789794921875\n",
      "Epoch: 8000\tTotal training loss: 0.8502647876739502\tTest mse: 52.61412811279297\n",
      "Epoch: 9000\tTotal training loss: 0.8502643704414368\tTest mse: 52.601112365722656\n",
      "Epoch: 10000\tTotal training loss: 0.8502642512321472\tTest mse: 52.63010025024414\n",
      "Epoch: 11000\tTotal training loss: 0.8502641916275024\tTest mse: 52.632625579833984\n",
      "Epoch: 12000\tTotal training loss: 0.8502641916275024\tTest mse: 52.626895904541016\n",
      "Epoch: 0\tTotal training loss: 0.8502960801124573\tTest mse: 52.510501861572266\n",
      "Epoch: 1000\tTotal training loss: 0.8502640724182129\tTest mse: 51.64584732055664\n",
      "Epoch: 2000\tTotal training loss: 0.8502640128135681\tTest mse: 51.64280319213867\n",
      "Epoch: 3000\tTotal training loss: 0.8502639532089233\tTest mse: 51.636478424072266\n",
      "Epoch: 4000\tTotal training loss: 0.8502640724182129\tTest mse: 51.630123138427734\n",
      "Epoch: 5000\tTotal training loss: 0.8502642512321472\tTest mse: 51.62288284301758\n",
      "Epoch: 6000\tTotal training loss: 0.8502641916275024\tTest mse: 51.62282180786133\n",
      "Epoch: 7000\tTotal training loss: 0.8502640724182129\tTest mse: 51.62395095825195\n",
      "Epoch: 8000\tTotal training loss: 0.8502640128135681\tTest mse: 51.621097564697266\n",
      "Epoch: 9000\tTotal training loss: 0.8502641320228577\tTest mse: 51.6147346496582\n",
      "Epoch: 10000\tTotal training loss: 0.8502641916275024\tTest mse: 51.60870361328125\n",
      "Epoch1 time per sample: 5.168001767585957e-06 seconds/sample\n",
      "Epoch2 : 5.476673869249427e-06 seconds/sample. N_sampel: 500\n",
      "error_expr_sorted [(51.60870361328125, -1.8635673522949219, 1.55201*x_1**4)]\n",
      "Constructing BFGS loss...\n",
      "Final expression:  1.55399291332699*x_1**4\n",
      "Test R2:  0.10188059018191353\n",
      "Test error:  0.8320540223088346\n",
      "Relative error:  1.188886330526345\n",
      "Reward:  0.5458354327017914\n",
      "\n",
      "\n",
      "Operators of each layer obtained by sampling:  {1: ['id', 'pow2', 'pow2'], 2: ['*', 'pow3', '+']}\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 1881.13916015625\tTest mse: 4603629.5\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 0.7522910237312317\tTest mse: 8.454657554626465\n",
      "Epoch: 1000\tTotal training loss: 0.04326629266142845\tTest mse: 0.0757652297616005\n",
      "Epoch: 2000\tTotal training loss: 0.04339181259274483\tTest mse: 0.06824599206447601\n",
      "Epoch: 3000\tTotal training loss: 0.04235231503844261\tTest mse: 0.09800810366868973\n",
      "Epoch: 4000\tTotal training loss: 0.04213399440050125\tTest mse: 0.10101060569286346\n",
      "Epoch: 5000\tTotal training loss: 0.041995856910943985\tTest mse: 0.10210252553224564\n",
      "Epoch: 6000\tTotal training loss: 0.04189832881093025\tTest mse: 0.1030484139919281\n",
      "Epoch: 7000\tTotal training loss: 0.041825663298368454\tTest mse: 0.10385698825120926\n",
      "Epoch: 8000\tTotal training loss: 0.04177048057317734\tTest mse: 0.1045437604188919\n",
      "Epoch: 9000\tTotal training loss: 0.04172832518815994\tTest mse: 0.10512147843837738\n",
      "Epoch: 10000\tTotal training loss: 0.04169601574540138\tTest mse: 0.10558884590864182\n",
      "Epoch: 11000\tTotal training loss: 0.04167111963033676\tTest mse: 0.10596335679292679\n",
      "Epoch: 12000\tTotal training loss: 0.041651807725429535\tTest mse: 0.10626120865345001\n",
      "Epoch: 0\tTotal training loss: 0.04180394858121872\tTest mse: 0.10253212600946426\n",
      "Epoch: 1000\tTotal training loss: 0.041641563177108765\tTest mse: 0.07688035070896149\n",
      "Epoch: 2000\tTotal training loss: 0.04163290560245514\tTest mse: 0.07689385116100311\n",
      "Epoch: 3000\tTotal training loss: 0.04162498563528061\tTest mse: 0.07690936326980591\n",
      "Epoch: 4000\tTotal training loss: 0.04161772131919861\tTest mse: 0.07692497968673706\n",
      "Epoch: 5000\tTotal training loss: 0.041611071676015854\tTest mse: 0.07693880796432495\n",
      "Epoch: 6000\tTotal training loss: 0.04160501807928085\tTest mse: 0.07695421576499939\n",
      "Epoch: 7000\tTotal training loss: 0.041599519550800323\tTest mse: 0.07696748524904251\n",
      "Epoch: 8000\tTotal training loss: 0.04159456118941307\tTest mse: 0.07698050141334534\n",
      "Epoch: 9000\tTotal training loss: 0.041590213775634766\tTest mse: 0.0770244225859642\n",
      "Epoch: 10000\tTotal training loss: 0.041586194187402725\tTest mse: 0.07703597098588943\n",
      "Epoch1 time per sample: 5.722022455897433e-06 seconds/sample\n",
      "Epoch2 : 5.735004798088916e-06 seconds/sample. N_sampel: 500\n",
      "error_expr_sorted [(0.07703597098588943, 0.9957255721092224, 1.02393*x_1**3 + 1.15419310996785*x_1**2 + 0.963359523429574*x_1)]\n",
      "Constructing BFGS loss...\n",
      "Final expression:  1.01371800267826*x_1**3 + 1.15700513037571*x_1**2 + 0.986737222362912*x_1\n",
      "Test R2:  0.9957846564177738\n",
      "Test error:  0.0039052642050298146\n",
      "Relative error:  0.8094918382999162\n",
      "Reward:  0.996109927555642\n",
      "\n",
      "\n",
      "~ Early Stopping Met ~\n",
      "Best expression:  1.01371800267826*x_1**3 + 1.15700513037571*x_1**2 + 0.986737222362912*x_1\n",
      "Best reward:      0.996109927555642\n",
      "mse error:       0.0039052642050298146\n",
      "Relative error:   0.8094918382999162\n",
      "Expression:  1.01371800267826*x_1**3 + 1.15700513037571*x_1**2 + 0.986737222362912*x_1\n",
      "R2:  0.9957846564177738\n",
      "Error:  0.0039052642050298146\n",
      "Relative Error:  0.8094918382999162\n",
      "log(1 + MSE):  0.003897658455983196\n",
      "IPS average:  6.613677233650206e-06\n"
     ]
    }
   ],
   "source": [
    "from DySymNetPP import SymbolicRegression\n",
    "from DySymNetPP.scripts.params import Params\n",
    "from DySymNetPP.scripts.functions import *\n",
    "\"\"\"Example 1: Input Ground Truth Expression, auto-generate data and extract expression\"\"\"\n",
    "import numpy as np\n",
    "# Define the hyperparameters for the symbolic neural network\n",
    "config = Params()\n",
    "# Define the functions available for symbolic regression\n",
    "funcs = [Identity(),\n",
    "         Square(),\n",
    "         Pow(3),\n",
    "         Plus(),\n",
    "         Product(),\n",
    "        ]\n",
    "config.funcs_avail = funcs\n",
    "config.N_TRAIN = 500\n",
    "#config.n_layers =[5]\n",
    "#config.num_func_layer = [6]\n",
    "config.use_gpu = False\n",
    "config.NOISE = 0.05\n",
    "\n",
    "SR = SymbolicRegression.SymboliRegression(config=config, func=\"x_1**3 + x_1**2 + x_1\", func_name=\"Nguyen-1\")\n",
    "eq, R2, error, relative_error, IPS_ave = SR.solve_environment()\n",
    "\n",
    "print('Expression: ', eq)\n",
    "print('R2: ', R2)\n",
    "print('Error: ', error)\n",
    "print('Relative Error: ', relative_error)\n",
    "print('log(1 + MSE): ', np.log(1+error))\n",
    "print('IPS average: ', IPS_ave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grep: warning: GREP_OPTIONS is deprecated; please use an alias or script\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use Device: gpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/shared/DySymNetPP/DySymNetPP/SymbolicRegression.py:27: SymPyDeprecationWarning: \n",
      "\n",
      "Passing the function arguments to lambdify() as a set is deprecated. This\n",
      "leads to unpredictable results since sets are unordered. Instead, use a list\n",
      "or tuple for the function arguments.\n",
      "\n",
      "See https://docs.sympy.org/latest/explanation/active-deprecations.html#deprecated-lambdify-arguments-set\n",
      "for details.\n",
      "\n",
      "This has been deprecated since SymPy version 1.6.3. It\n",
      "will be removed in a future version of SymPy.\n",
      "\n",
      "  sp_expr = sp.lambdify(free_symbols, func)\n",
      "W0126 06:59:37.422758 28641 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 12.0, Runtime API Version: 11.8\n",
      "W0126 06:59:37.424078 28641 gpu_resources.cc:164] device: 0, cuDNN Version: 8.9.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************* Start Sampling... *************************\n",
      "\n",
      "******************** Epoch 00 ********************\n",
      "Operators of each layer obtained by sampling:  {1: ['*', '+', 'pow2', 'pow2', 'pow2'], 2: ['+', 'id', '*', '*', '*'], 3: ['+', '*', '+', 'pow3', 'id']}\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 8.379371078406478e+19\tTest mse: 9.922990631291914e+25\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 21774842462208.0\tTest mse: 5.748165357432996e+17\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 9.068772764889907e+16\tTest mse: 2.2287957082636162e+21\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 3630131323076608.0\tTest mse: 1.5605594205587346e+22\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 1497667846799360.0\tTest mse: 7.4619607423446e+20\n",
      "Final expression:  None\n",
      "Test R2:  0.0\n",
      "Test error:  10000\n",
      "Relative error:  100\n",
      "Reward:  9.999000099990002e-05\n",
      "\n",
      "\n",
      "Operators of each layer obtained by sampling:  {1: ['pow2', 'pow3', 'pow2', 'id', 'id'], 2: ['pow2', '*', '*', 'pow2', 'id'], 3: ['pow3', '*', 'id', 'pow3', 'id'], 4: ['pow2', 'pow2', 'pow3', '*', '*']}\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Final expression:  None\n",
      "Test R2:  0.0\n",
      "Test error:  10000\n",
      "Relative error:  100\n",
      "Reward:  9.999000099990002e-05\n",
      "\n",
      "\n",
      "Operators of each layer obtained by sampling:  {1: ['id', 'pow3', '*', '+'], 2: ['pow3', '*', '+', 'pow3'], 3: ['pow2', '*', 'pow3', 'pow3']}\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 1.4284085033777975e+34\tTest mse: inf\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 136023520.0\tTest mse: 26826094592.0\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 10654950.0\tTest mse: 37750177792.0\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 6.096170064753459e+17\tTest mse: 2.5563364530493556e+28\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 3.102989070401424e+31\tTest mse: inf\n",
      "Final expression:  None\n",
      "Test R2:  0.0\n",
      "Test error:  10000\n",
      "Relative error:  100\n",
      "Reward:  9.999000099990002e-05\n",
      "\n",
      "\n",
      "Operators of each layer obtained by sampling:  {1: ['pow2', 'id', 'pow3', '*', 'pow2', '+'], 2: ['+', '*', '+', '*', 'pow2', 'id'], 3: ['id', 'pow3', '+', '*', 'pow3', 'id'], 4: ['*', 'pow2', 'pow2', 'pow2', 'pow3', 'pow2'], 5: ['*', 'pow2', 'pow2', 'pow2', '+', 'pow2']}\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Final expression:  None\n",
      "Test R2:  0.0\n",
      "Test error:  10000\n",
      "Relative error:  100\n",
      "Reward:  9.999000099990002e-05\n",
      "\n",
      "\n",
      "Operators of each layer obtained by sampling:  {1: ['pow3', 'pow3', '*', 'pow3'], 2: ['pow2', 'id', 'pow2', '*']}\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 1.021998643875122\tTest mse: 190.18031311035156\n",
      "Epoch: 1000\tTotal training loss: 0.10097645223140717\tTest mse: 251.92518615722656\n",
      "Epoch: 2000\tTotal training loss: 0.08781009912490845\tTest mse: 199.02835083007812\n",
      "Epoch: 3000\tTotal training loss: 0.06928807497024536\tTest mse: 265.0465087890625\n",
      "Epoch: 4000\tTotal training loss: 0.069040447473526\tTest mse: 271.143798828125\n",
      "Epoch: 5000\tTotal training loss: 0.0679563656449318\tTest mse: 281.00213623046875\n",
      "Epoch: 6000\tTotal training loss: 0.06720523536205292\tTest mse: 281.2075500488281\n",
      "Epoch: 7000\tTotal training loss: 0.06654965877532959\tTest mse: 281.33148193359375\n",
      "Epoch: 8000\tTotal training loss: 0.06600090116262436\tTest mse: 281.2088623046875\n",
      "Epoch: 9000\tTotal training loss: 0.06554117053747177\tTest mse: 280.1918640136719\n",
      "Epoch: 10000\tTotal training loss: 0.06514229625463486\tTest mse: 281.90972900390625\n",
      "Epoch: 11000\tTotal training loss: 0.06479468941688538\tTest mse: 282.5724182128906\n",
      "Epoch: 12000\tTotal training loss: 0.06448855996131897\tTest mse: 283.6091003417969\n",
      "Epoch: 0\tTotal training loss: 0.06515553593635559\tTest mse: 282.0782470703125\n",
      "Epoch: 1000\tTotal training loss: 0.06441056728363037\tTest mse: 271.62786865234375\n",
      "Epoch: 2000\tTotal training loss: 0.06433973461389542\tTest mse: 271.82958984375\n",
      "Epoch: 3000\tTotal training loss: 0.0642724484205246\tTest mse: 272.04656982421875\n",
      "Epoch: 4000\tTotal training loss: 0.06420861184597015\tTest mse: 272.2655944824219\n",
      "Epoch: 5000\tTotal training loss: 0.06414808332920074\tTest mse: 272.48553466796875\n",
      "Epoch: 6000\tTotal training loss: 0.06409068405628204\tTest mse: 272.7081604003906\n",
      "Epoch: 7000\tTotal training loss: 0.06403617560863495\tTest mse: 272.9239501953125\n",
      "Epoch: 8000\tTotal training loss: 0.06398425996303558\tTest mse: 273.13018798828125\n",
      "Epoch: 9000\tTotal training loss: 0.06393460929393768\tTest mse: 273.335205078125\n",
      "Epoch: 10000\tTotal training loss: 0.0638868510723114\tTest mse: 273.5337829589844\n",
      "Epoch1 time per sample: 9.095441678217793e-06 seconds/sample\n",
      "Epoch2 : 9.080428264699165e-06 seconds/sample. N_sampel: 500\n",
      "error_expr_sorted [(273.5337829589844, -15.541120529174805, -2.67289311332123*x_1**5 + 4.4744*x_1**3 + 0.835040963219771*x_1**2)]\n",
      "Constructing BFGS loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0126 07:01:26.298415 28641 dygraph_functions.cc:83028] got different data type, run type promotion automatically, this may cause data type been changed.\n",
      "W0126 07:01:26.299113 28641 dygraph_functions.cc:90372] got different data type, run type promotion automatically, this may cause data type been changed.\n",
      "W0126 07:01:26.299401 28641 dygraph_functions.cc:85677] got different data type, run type promotion automatically, this may cause data type been changed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final expression:  -2.72406152622187*x_1**5 + 4.51953697665263*x_1**3 + 0.931689558607487*x_1**2\n",
      "Test R2:  0.9834168510824829\n",
      "Test error:  0.017864346970347913\n",
      "Relative error:  0.3939641878886261\n",
      "Reward:  0.9824491868454566\n",
      "\n",
      "\n",
      "Operators of each layer obtained by sampling:  {1: ['+', 'pow3', 'pow3', 'pow2', 'id'], 2: ['+', 'pow2', 'id', 'pow3', 'pow2'], 3: ['*', 'pow3', 'pow3', 'pow3', 'pow3'], 4: ['*', 'pow3', 'pow3', 'pow2', 'pow3']}\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Final expression:  None\n",
      "Test R2:  0.0\n",
      "Test error:  10000\n",
      "Relative error:  100\n",
      "Reward:  9.999000099990002e-05\n",
      "\n",
      "\n",
      "Operators of each layer obtained by sampling:  {1: ['id', 'pow2'], 2: ['pow3', '+']}\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 1.376542091369629\tTest mse: 23.298887252807617\n",
      "Epoch: 1000\tTotal training loss: 0.04082246124744415\tTest mse: 0.0842546820640564\n",
      "Epoch: 2000\tTotal training loss: 0.04001195356249809\tTest mse: 0.04449668526649475\n",
      "Epoch: 3000\tTotal training loss: 0.03520210087299347\tTest mse: 0.04267946258187294\n",
      "Epoch: 4000\tTotal training loss: 0.034993719309568405\tTest mse: 0.048982828855514526\n",
      "Epoch: 5000\tTotal training loss: 0.03484916687011719\tTest mse: 0.05255688354372978\n",
      "Epoch: 6000\tTotal training loss: 0.03474351018667221\tTest mse: 0.0542382188141346\n",
      "Epoch: 7000\tTotal training loss: 0.03466513380408287\tTest mse: 0.05491798371076584\n",
      "Epoch: 8000\tTotal training loss: 0.03460581973195076\tTest mse: 0.05523448437452316\n",
      "Epoch: 9000\tTotal training loss: 0.03456036001443863\tTest mse: 0.055559463798999786\n",
      "Epoch: 10000\tTotal training loss: 0.034525614231824875\tTest mse: 0.056095995008945465\n",
      "Epoch: 11000\tTotal training loss: 0.03449968993663788\tTest mse: 0.05696825683116913\n",
      "Epoch: 12000\tTotal training loss: 0.0344817154109478\tTest mse: 0.05831759050488472\n",
      "Epoch: 0\tTotal training loss: 0.03443601727485657\tTest mse: 0.060007840394973755\n",
      "Epoch: 1000\tTotal training loss: 0.0344088077545166\tTest mse: 0.06607282906770706\n",
      "Epoch: 2000\tTotal training loss: 0.034393440932035446\tTest mse: 0.06694234907627106\n",
      "Epoch: 3000\tTotal training loss: 0.03438025712966919\tTest mse: 0.06770716607570648\n",
      "Epoch: 4000\tTotal training loss: 0.034368883818387985\tTest mse: 0.06841070204973221\n",
      "Epoch: 5000\tTotal training loss: 0.03435901179909706\tTest mse: 0.06907182931900024\n",
      "Epoch: 6000\tTotal training loss: 0.03435038775205612\tTest mse: 0.06970932334661484\n",
      "Epoch: 7000\tTotal training loss: 0.03434281796216965\tTest mse: 0.07032734900712967\n",
      "Epoch: 8000\tTotal training loss: 0.0343361422419548\tTest mse: 0.07093898952007294\n",
      "Epoch: 9000\tTotal training loss: 0.03433022275567055\tTest mse: 0.07155343890190125\n",
      "Epoch: 10000\tTotal training loss: 0.034324973821640015\tTest mse: 0.07215727120637894\n",
      "Epoch1 time per sample: 8.028113954256559e-06 seconds/sample\n",
      "Epoch2 : 8.098911302183381e-06 seconds/sample. N_sampel: 500\n",
      "error_expr_sorted [(0.07215727120637894, 0.995636522769928, 1.01179*x_1**3 + 0.8226892314932*x_1**2 + 0.969087580750795*x_1)]\n",
      "Constructing BFGS loss...\n",
      "Final expression:  1.01028296755951*x_1**3 + 0.927598577155303*x_1**2 + 0.993856023120202*x_1\n",
      "Test R2:  0.9991460945245924\n",
      "Test error:  0.0009198773868844526\n",
      "Relative error:  0.21585458129298024\n",
      "Reward:  0.9990809680098611\n",
      "\n",
      "\n",
      "~ Early Stopping Met ~\n",
      "Best expression:  1.01028296755951*x_1**3 + 0.927598577155303*x_1**2 + 0.993856023120202*x_1\n",
      "Best reward:      0.9990809680098611\n",
      "mse error:       0.0009198773868844526\n",
      "Relative error:   0.21585458129298024\n",
      "Expression:  1.01028296755951*x_1**3 + 0.927598577155303*x_1**2 + 0.993856023120202*x_1\n",
      "R2:  0.9991460945245924\n",
      "Error:  0.0009198773868844526\n",
      "Relative Error:  0.21585458129298024\n",
      "log(1 + MSE):  0.0009194545589609502\n",
      "IPS average:  8.575723799839225e-06\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Example 1: Input Ground Truth Expression, auto-generate data and extract expression\n",
    "   to use gpu should create a new environment to install paddlepaddle-gpu\n",
    "\"\"\"\n",
    "from DySymNetPP import SymbolicRegression\n",
    "from DySymNetPP.scripts.params import Params\n",
    "from DySymNetPP.scripts.functions import *\n",
    "import numpy as np\n",
    "# Define the hyperparameters for the symbolic neural network\n",
    "config = Params()\n",
    "# Define the functions available for symbolic regression\n",
    "funcs = [Identity(),\n",
    "         Square(),\n",
    "         Pow(3),\n",
    "         Plus(),\n",
    "         Product(),\n",
    "        ]\n",
    "config.funcs_avail = funcs\n",
    "config.N_TRAIN = 500\n",
    "#config.n_layers =[5]\n",
    "#config.num_func_layer = [6]\n",
    "config.use_gpu = True\n",
    "config.NOISE = 0.05\n",
    "\n",
    "SR = SymbolicRegression.SymboliRegression(config=config, func=\"x_1**3 + x_1**2 + x_1\", func_name=\"Nguyen-1\")\n",
    "eq, R2, error, relative_error, IPS_ave = SR.solve_environment()\n",
    "\n",
    "print('Expression: ', eq)\n",
    "print('R2: ', R2)\n",
    "print('Error: ', error)\n",
    "print('Relative Error: ', relative_error)\n",
    "print('log(1 + MSE): ', np.log(1+error))\n",
    "print('IPS average: ', IPS_ave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use Device: cuda\n",
      "************************* Start Sampling... *************************\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/shared/envs/DySym/lib/python3.10/site-packages/DySymNet/SymbolicRegression.py:30: SymPyDeprecationWarning: \n",
      "\n",
      "Passing the function arguments to lambdify() as a set is deprecated. This\n",
      "leads to unpredictable results since sets are unordered. Instead, use a list\n",
      "or tuple for the function arguments.\n",
      "\n",
      "See https://docs.sympy.org/latest/explanation/active-deprecations.html#deprecated-lambdify-arguments-set\n",
      "for details.\n",
      "\n",
      "This has been deprecated since SymPy version 1.6.3. It\n",
      "will be removed in a future version of SymPy.\n",
      "\n",
      "  sp_expr = sp.lambdify(free_symbols, func)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************** Epoch 00 ********************\n",
      "Operators of each layer obtained by sampling:  {1: ['id', '*', 'pow2', 'id'], 2: ['pow3', 'pow2', '+', 'pow2'], 3: ['+', 'pow3', 'id', 'id'], 4: ['+', 'pow2', '*', 'id'], 5: ['id', 'id', 'pow2', '*']}\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 1.3452985286712646\tTest mse: 1.0577213764190674\n",
      "Epoch: 1000\tTotal training loss: 0.13294364511966705\tTest mse: 0.0038859560154378414\n",
      "Epoch: 2000\tTotal training loss: 0.12174498289823532\tTest mse: 0.0039010809268802404\n",
      "Epoch: 3000\tTotal training loss: 0.10262516885995865\tTest mse: 0.0059630367904901505\n",
      "Epoch: 4000\tTotal training loss: 0.10227345675230026\tTest mse: 0.00565507635474205\n",
      "Epoch: 5000\tTotal training loss: 0.10210495442152023\tTest mse: 0.0052400194108486176\n",
      "Epoch: 6000\tTotal training loss: 0.10176802426576614\tTest mse: 0.005093346815556288\n",
      "Epoch: 7000\tTotal training loss: 0.10142147541046143\tTest mse: 0.005102277733385563\n",
      "Epoch: 8000\tTotal training loss: 0.10114327818155289\tTest mse: 0.005165807902812958\n",
      "Epoch: 9000\tTotal training loss: 0.10110551863908768\tTest mse: 0.0050431787967681885\n",
      "Epoch: 10000\tTotal training loss: 0.10098616778850555\tTest mse: 0.005117407999932766\n",
      "Epoch: 11000\tTotal training loss: 0.10092860460281372\tTest mse: 0.005150520708411932\n",
      "Epoch: 12000\tTotal training loss: 0.1009133830666542\tTest mse: 0.005131156649440527\n",
      "Epoch: 0\tTotal training loss: 0.1003284752368927\tTest mse: 0.004950972739607096\n",
      "Epoch: 1000\tTotal training loss: 0.10028951615095139\tTest mse: 0.004988315515220165\n",
      "Epoch: 2000\tTotal training loss: 0.10025890171527863\tTest mse: 0.0049864258617162704\n",
      "Epoch: 3000\tTotal training loss: 0.10023301094770432\tTest mse: 0.004985620733350515\n",
      "Epoch: 4000\tTotal training loss: 0.10021200031042099\tTest mse: 0.0049970801919698715\n",
      "Epoch: 5000\tTotal training loss: 0.10019563138484955\tTest mse: 0.004994858056306839\n",
      "Epoch: 6000\tTotal training loss: 0.10018236190080643\tTest mse: 0.005000835284590721\n",
      "Epoch: 7000\tTotal training loss: 0.10017099976539612\tTest mse: 0.005014584865421057\n",
      "Epoch: 8000\tTotal training loss: 0.10016211867332458\tTest mse: 0.005011416040360928\n",
      "Epoch: 9000\tTotal training loss: 0.10015417635440826\tTest mse: 0.005012966226786375\n",
      "Epoch: 10000\tTotal training loss: 0.10014886409044266\tTest mse: 0.004995384719222784\n",
      "Epoch1 time per sample: 1.813963621559267e-05 seconds/sample\n",
      "Epoch2 : 1.7675834516920624e-05 seconds/sample. N_sampel: 500\n",
      "error_expr_sorted [(0.004995384719222784, 0.9948826432228088, 1.00083020865146*x_1 + 0.590286711260022*(-0.826217399992919*x_1**2 - x_1)**2)]\n",
      "Constructing BFGS loss...\n",
      "Final expression:  1.01496256742242*x_1 + 0.575155981776112*(-0.846376761738959*x_1**2 - x_1)**2\n",
      "Test R2:  0.9977453947067261\n",
      "Test error:  0.002101413207128644\n",
      "Relative error:  0.3880212903022766\n",
      "Reward:  0.9979029934700887\n",
      "\n",
      "\n",
      "~ Early Stopping Met ~\n",
      "Best expression:  1.01496256742242*x_1 + 0.575155981776112*(-0.846376761738959*x_1**2 - x_1)**2\n",
      "Best reward:      0.9979029934700887\n",
      "mse error:       0.002101413207128644\n",
      "Relative error:   0.3880212903022766\n",
      "IPS average:  1.7907735366256647e-05\n",
      "\n",
      "\n",
      "Expression:  1.01496256742242*x_1 + 0.575155981776112*(-0.846376761738959*x_1**2 - x_1)**2\n",
      "R2:  0.9977453947067261\n",
      "Error:  0.002101413207128644\n",
      "Relative Error:  0.3880212903022766\n",
      "log(1 + MSE):  0.0020992083267645906\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Torch test\n",
    "envs: /shared/envs/DySym\"\"\"\n",
    "from DySymNet import SymbolicRegression\n",
    "from DySymNet.scripts.params import Params\n",
    "from DySymNet.scripts.functions import *\n",
    "\n",
    "import numpy as np\n",
    "# Define the hyperparameters for the symbolic neural network\n",
    "config = Params()\n",
    "# Define the functions available for symbolic regression\n",
    "funcs = [Identity(),\n",
    "         Square(),\n",
    "         Pow(3),\n",
    "         Plus(),\n",
    "         Product(),\n",
    "        ]\n",
    "config.funcs_avail = funcs\n",
    "config.N_TRAIN = 500\n",
    "#config.n_layers =[5]\n",
    "#config.num_func_layer = [6]\n",
    "config.use_gpu = True\n",
    "config.NOISE = 0.05\n",
    "\n",
    "SR = SymbolicRegression.SymboliRegression(config=config, func=\"x_1**3 + x_1**2 + x_1\", func_name=\"Nguyen-1\")\n",
    "eq, R2, error, relative_error = SR.solve_environment()\n",
    "\n",
    "print('Expression: ', eq)\n",
    "print('R2: ', R2)\n",
    "print('Error: ', error)\n",
    "print('Relative Error: ', relative_error)\n",
    "print('log(1 + MSE): ', np.log(1+error))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
