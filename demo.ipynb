{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grep: warning: GREP_OPTIONS is deprecated; please use an alias or script\n"
     ]
    }
   ],
   "source": [
    "from DySymNetPP import SymbolicRegression\n",
    "from DySymNetPP.scripts.params import Params\n",
    "from DySymNetPP.scripts.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use Device: Place(cpu)\n",
      "************************* Start Sampling... *************************\n",
      "\n",
      "******************** Epoch 00 ********************\n",
      "Operators of each layer obtained by sampling:  {1: ['id', '+'], 2: ['pow2', '/'], 3: ['-', '-']}\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 17.443613052368164\tTest mse: 41.93836975097656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/shared/DySymNetPP/DySymNetPP/SymbolicRegression.py:27: SymPyDeprecationWarning: \n",
      "\n",
      "Passing the function arguments to lambdify() as a set is deprecated. This\n",
      "leads to unpredictable results since sets are unordered. Instead, use a list\n",
      "or tuple for the function arguments.\n",
      "\n",
      "See https://docs.sympy.org/latest/explanation/active-deprecations.html#deprecated-lambdify-arguments-set\n",
      "for details.\n",
      "\n",
      "This has been deprecated since SymPy version 1.6.3. It\n",
      "will be removed in a future version of SymPy.\n",
      "\n",
      "  sp_expr = sp.lambdify(free_symbols, func)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1000\tTotal training loss: 0.8078905344009399\tTest mse: 20.606882095336914\n",
      "Epoch: 2000\tTotal training loss: 0.80765700340271\tTest mse: 20.36878776550293\n",
      "Epoch: 3000\tTotal training loss: 0.8051750063896179\tTest mse: 20.511085510253906\n",
      "Epoch: 4000\tTotal training loss: 0.805155336856842\tTest mse: 20.512598037719727\n",
      "Epoch: 5000\tTotal training loss: 0.8051468133926392\tTest mse: 20.513357162475586\n",
      "Epoch: 6000\tTotal training loss: 0.805138349533081\tTest mse: 20.514209747314453\n",
      "Epoch: 7000\tTotal training loss: 0.8051410913467407\tTest mse: 20.514039993286133\n",
      "Epoch: 8000\tTotal training loss: 0.805134117603302\tTest mse: 20.514612197875977\n",
      "Epoch: 9000\tTotal training loss: 0.805132269859314\tTest mse: 20.514833450317383\n",
      "Epoch: 10000\tTotal training loss: 0.8051257133483887\tTest mse: 20.51552963256836\n",
      "Epoch: 11000\tTotal training loss: 0.8051245808601379\tTest mse: 20.515710830688477\n",
      "Epoch: 12000\tTotal training loss: 0.805121660232544\tTest mse: 20.515853881835938\n",
      "Epoch: 0\tTotal training loss: 0.8050578236579895\tTest mse: 20.509057998657227\n",
      "Epoch: 1000\tTotal training loss: 0.804928719997406\tTest mse: 20.494548797607422\n",
      "Epoch: 2000\tTotal training loss: 0.8049283623695374\tTest mse: 20.494733810424805\n",
      "Epoch: 3000\tTotal training loss: 0.8049284219741821\tTest mse: 20.494333267211914\n",
      "Epoch: 4000\tTotal training loss: 0.8049286007881165\tTest mse: 20.494047164916992\n",
      "Epoch: 5000\tTotal training loss: 0.8049285411834717\tTest mse: 20.493825912475586\n",
      "Epoch: 6000\tTotal training loss: 0.8049286007881165\tTest mse: 20.493642807006836\n",
      "Epoch: 7000\tTotal training loss: 0.804928719997406\tTest mse: 20.493471145629883\n",
      "Epoch: 8000\tTotal training loss: 0.8049286603927612\tTest mse: 20.493345260620117\n",
      "Epoch: 9000\tTotal training loss: 0.8049288392066956\tTest mse: 20.493236541748047\n",
      "Epoch: 10000\tTotal training loss: 0.8049287796020508\tTest mse: 20.493139266967773\n",
      "error_expr_sorted [(20.493139266967773, -0.14456450939178467, 0)]\n",
      "Final expression:  0\n",
      "Test R2:  0.0\n",
      "Test error:  20.493139266967773\n",
      "Relative error:  100\n",
      "Reward:  0.046526474684732214\n",
      "\n",
      "\n",
      "Operators of each layer obtained by sampling:  {1: ['-', 'id'], 2: ['pow3', 'pow3'], 3: ['*', '/'], 4: ['+', 'pow2'], 5: ['id', '*']}\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 2.609776536546121e+22\tTest mse: inf\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 5.616194243589205e+25\tTest mse: inf\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 5.969202516135455e+28\tTest mse: inf\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 1.9260874189868124e+23\tTest mse: inf\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 6151.740234375\tTest mse: 3207684.75\n",
      "Final expression:  None\n",
      "Test R2:  0.0\n",
      "Test error:  10000\n",
      "Relative error:  100\n",
      "Reward:  9.999000099990002e-05\n",
      "\n",
      "\n",
      "Operators of each layer obtained by sampling:  {1: ['/', '/', '/'], 2: ['+', '+', 'id'], 3: ['pow3', 'id', 'pow2'], 4: ['pow2', '-', '/']}\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 1.2237986151104486e+23\tTest mse: 1.2157244716265062e+23\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 1.8697279068092498e+18\tTest mse: 1.8696043491900785e+18\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 4.472997839855449e+35\tTest mse: nan\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 129580.359375\tTest mse: 19.939729690551758\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Final expression:  None\n",
      "Test R2:  0.0\n",
      "Test error:  10000\n",
      "Relative error:  100\n",
      "Reward:  9.999000099990002e-05\n",
      "\n",
      "\n",
      "Operators of each layer obtained by sampling:  {1: ['id', '*', '/', 'pow3', 'id'], 2: ['*', 'pow2', 'pow2', 'pow3', 'pow3']}\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 105388384.0\tTest mse: 37948340.0\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 494470430720.0\tTest mse: 1675214848.0\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 8893906944.0\tTest mse: 11818574544896.0\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 28508304.0\tTest mse: 97702969344.0\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 4113951.0\tTest mse: 185966304.0\n",
      "Final expression:  None\n",
      "Test R2:  0.0\n",
      "Test error:  10000\n",
      "Relative error:  100\n",
      "Reward:  9.999000099990002e-05\n",
      "\n",
      "\n",
      "Operators of each layer obtained by sampling:  {1: ['+', 'pow3', '*', '*', 'pow3', 'pow2'], 2: ['*', '*', 'pow3', '-', 'pow3', 'id']}\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 4244.5478515625\tTest mse: 555083.5625\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 165865.671875\tTest mse: 137106304.0\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 88269.4609375\tTest mse: 4703462400.0\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 967480.3125\tTest mse: 50796109824.0\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 2261210112.0\tTest mse: 6071674470400.0\n",
      "Final expression:  None\n",
      "Test R2:  0.0\n",
      "Test error:  10000\n",
      "Relative error:  100\n",
      "Reward:  9.999000099990002e-05\n",
      "\n",
      "\n",
      "Operators of each layer obtained by sampling:  {1: ['pow3', '-', 'pow3', '/'], 2: ['-', 'id', 'id', 'pow3'], 3: ['+', '-', '+', '+'], 4: ['pow2', 'id', '*', '*'], 5: ['*', '-', '-', '/']}\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: inf\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 4.627115234563884e+35\tTest mse: inf\n",
      "Final expression:  None\n",
      "Test R2:  0.0\n",
      "Test error:  10000\n",
      "Relative error:  100\n",
      "Reward:  9.999000099990002e-05\n",
      "\n",
      "\n",
      "Operators of each layer obtained by sampling:  {1: ['-', '*', '+'], 2: ['-', '+', 'pow2'], 3: ['/', '/', 'pow3']}\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 1.144732012149683e+19\tTest mse: 2.8228760997441362e+26\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 16406996320256.0\tTest mse: 2.306058073688087e+19\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 2163511794860032.0\tTest mse: 8.620058571773156e+21\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 1644.0496826171875\tTest mse: 591376.9375\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 98183928.0\tTest mse: 762158579712.0\n",
      "Final expression:  None\n",
      "Test R2:  0.0\n",
      "Test error:  10000\n",
      "Relative error:  100\n",
      "Reward:  9.999000099990002e-05\n",
      "\n",
      "\n",
      "Operators of each layer obtained by sampling:  {1: ['*', 'pow2'], 2: ['*', 'pow3'], 3: ['-', '-']}\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 0.8872825503349304\tTest mse: 3803.634765625\n",
      "Epoch: 1000\tTotal training loss: 0.8305932283401489\tTest mse: 14.947807312011719\n",
      "Epoch: 2000\tTotal training loss: 0.8304946422576904\tTest mse: 14.960221290588379\n",
      "Epoch: 3000\tTotal training loss: 0.8293631672859192\tTest mse: 14.907204627990723\n",
      "Epoch: 4000\tTotal training loss: 0.8293631076812744\tTest mse: 14.906854629516602\n",
      "Epoch: 5000\tTotal training loss: 0.8293635249137878\tTest mse: 14.905766487121582\n",
      "Epoch: 6000\tTotal training loss: 0.8293642997741699\tTest mse: 14.904790878295898\n",
      "Epoch: 7000\tTotal training loss: 0.8293647170066833\tTest mse: 14.904016494750977\n",
      "Epoch: 8000\tTotal training loss: 0.8293651938438416\tTest mse: 14.903423309326172\n",
      "Epoch: 9000\tTotal training loss: 0.829365611076355\tTest mse: 14.902933120727539\n",
      "Epoch: 10000\tTotal training loss: 0.8293659687042236\tTest mse: 14.9025297164917\n",
      "Epoch: 11000\tTotal training loss: 0.8293662071228027\tTest mse: 14.902200698852539\n",
      "Epoch: 12000\tTotal training loss: 0.8293665051460266\tTest mse: 14.901920318603516\n",
      "Epoch: 0\tTotal training loss: 0.8293732404708862\tTest mse: 14.89992904663086\n",
      "Epoch: 1000\tTotal training loss: 0.8293625116348267\tTest mse: 14.891559600830078\n",
      "Epoch: 2000\tTotal training loss: 0.8293625116348267\tTest mse: 14.891429901123047\n",
      "Epoch: 3000\tTotal training loss: 0.8293625116348267\tTest mse: 14.891349792480469\n",
      "Epoch: 4000\tTotal training loss: 0.8293624520301819\tTest mse: 14.891305923461914\n",
      "Epoch: 5000\tTotal training loss: 0.8293623924255371\tTest mse: 14.891291618347168\n",
      "Epoch: 6000\tTotal training loss: 0.8293625116348267\tTest mse: 14.891290664672852\n",
      "Epoch: 7000\tTotal training loss: 0.8293625116348267\tTest mse: 14.891290664672852\n",
      "Epoch: 8000\tTotal training loss: 0.8293625116348267\tTest mse: 14.891290664672852\n",
      "Epoch: 9000\tTotal training loss: 0.8293625116348267\tTest mse: 14.891290664672852\n",
      "Epoch: 10000\tTotal training loss: 0.8293625116348267\tTest mse: 14.891290664672852\n",
      "error_expr_sorted [(14.891290664672852, 0.16830503940582275, 0.493432096665935*x_1**4)]\n",
      "Constructing BFGS loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0124 07:22:10.933949  7241 dygraph_functions.cc:81971] got different data type, run type promotion automatically, this may cause data type been changed.\n",
      "W0124 07:22:10.935425  7241 dygraph_functions.cc:87724] got different data type, run type promotion automatically, this may cause data type been changed.\n",
      "W0124 07:22:10.935571  7241 dygraph_functions.cc:83253] got different data type, run type promotion automatically, this may cause data type been changed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final expression:  0.539168083951196*x_1**4\n",
      "Test R2:  0.0\n",
      "Test error:  0.8028336911226933\n",
      "Relative error:  1.091413092614937\n",
      "Reward:  0.5546823342186721\n",
      "\n",
      "\n",
      "Operators of each layer obtained by sampling:  {1: ['pow2', 'pow3', '/', 'pow2'], 2: ['+', 'id', 'pow2', '*'], 3: ['*', 'id', '+', '/'], 4: ['pow2', '*', '+', '-']}\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 3.8381192836805865e+23\tTest mse: 1.3033487073361244e+26\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 4.986528285223744e+26\tTest mse: 1.7730250234775426e+36\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 8.555468468334965e+24\tTest mse: inf\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 9.746422529089038e+27\tTest mse: inf\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 1.234322950978861e+25\tTest mse: 1.2567513783857663e+30\n",
      "Final expression:  None\n",
      "Test R2:  0.0\n",
      "Test error:  10000\n",
      "Relative error:  100\n",
      "Reward:  9.999000099990002e-05\n",
      "\n",
      "\n",
      "Operators of each layer obtained by sampling:  {1: ['-', 'pow2', 'pow3', 'id'], 2: ['pow3', '-', '+', 'pow2'], 3: ['/', '+', '+', '*'], 4: ['pow3', '-', '*', 'pow2'], 5: ['+', '-', 'pow2', '+']}\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Final expression:  None\n",
      "Test R2:  0.0\n",
      "Test error:  10000\n",
      "Relative error:  100\n",
      "Reward:  9.999000099990002e-05\n",
      "\n",
      "\n",
      "******************** Epoch 01 ********************\n",
      "Operators of each layer obtained by sampling:  {1: ['id', '-', 'pow3', '*'], 2: ['*', '*', '*', '/'], 3: ['/', 'id', '/', '*'], 4: ['+', '+', '/', '-']}\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 1.505341025990738e+17\tTest mse: 1.9135931332060942e+21\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 460815530459136.0\tTest mse: 4.7059998388738274e+20\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 26800226.0\tTest mse: 53564194816.0\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 421312167936.0\tTest mse: 115339157307392.0\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 18589056761856.0\tTest mse: 1.326594863772205e+18\n",
      "Final expression:  None\n",
      "Test R2:  0.0\n",
      "Test error:  10000\n",
      "Relative error:  100\n",
      "Reward:  9.999000099990002e-05\n",
      "\n",
      "\n",
      "Operators of each layer obtained by sampling:  {1: ['pow2', '+', '+', '-', 'pow2'], 2: ['*', '+', '/', 'pow3', '*'], 3: ['-', 'pow3', '*', 'id', '/'], 4: ['/', 'pow3', '*', '+', 'pow2']}\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 2.1550007011143837e+31\tTest mse: inf\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 5.748203179405936e+24\tTest mse: 3.226641590936585e+35\n",
      "Final expression:  None\n",
      "Test R2:  0.0\n",
      "Test error:  10000\n",
      "Relative error:  100\n",
      "Reward:  9.999000099990002e-05\n",
      "\n",
      "\n",
      "Operators of each layer obtained by sampling:  {1: ['+', '/', 'pow3', 'id', 'pow3'], 2: ['+', 'pow2', '+', '/', '/'], 3: ['pow2', '*', '+', 'pow2', '*']}\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 16967777280.0\tTest mse: 4313300480.0\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 256265936.0\tTest mse: 2275512156160.0\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 1474806784.0\tTest mse: 6496883748896768.0\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 93397.4453125\tTest mse: 1288923904.0\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 2675715.5\tTest mse: 45740816.0\n",
      "Final expression:  None\n",
      "Test R2:  0.0\n",
      "Test error:  10000\n",
      "Relative error:  100\n",
      "Reward:  9.999000099990002e-05\n",
      "\n",
      "\n",
      "Operators of each layer obtained by sampling:  {1: ['*', '-', 'id', '+', '-'], 2: ['/', '*', '/', 'id', '*'], 3: ['/', 'pow2', '-', '+', '-'], 4: ['+', 'pow2', '*', '+', '/']}\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 3.727819269744886e+17\tTest mse: 3.967855739154144e+23\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 6.899743877305991e+18\tTest mse: 4.904017336370986e+24\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 2.1322378004178533e+22\tTest mse: 1.6733245201316508e+27\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 7.733761409440549e+29\tTest mse: inf\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 49017228.0\tTest mse: 91757387710464.0\n",
      "Final expression:  None\n",
      "Test R2:  0.0\n",
      "Test error:  10000\n",
      "Relative error:  100\n",
      "Reward:  9.999000099990002e-05\n",
      "\n",
      "\n",
      "Operators of each layer obtained by sampling:  {1: ['+', 'id', '+'], 2: ['/', '/', '-'], 3: ['pow3', '+', '+'], 4: ['pow2', '+', 'pow3'], 5: ['*', 'pow2', 'id']}\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 70861063192576.0\tTest mse: 1.9139643107394454e+19\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 1.0365456079318318e+26\tTest mse: 6.826858031126772e+34\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 1.4529578868618035e+17\tTest mse: 5.854900121594642e+20\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 2.8833591513964578e+28\tTest mse: inf\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 9.8661117469999e+34\tTest mse: inf\n",
      "Final expression:  None\n",
      "Test R2:  0.0\n",
      "Test error:  10000\n",
      "Relative error:  100\n",
      "Reward:  9.999000099990002e-05\n",
      "\n",
      "\n",
      "Operators of each layer obtained by sampling:  {1: ['*', '*', 'pow2', 'id', '*', '/'], 2: ['id', '+', '/', '*', '*', '/'], 3: ['-', 'pow2', 'pow3', 'id', 'pow2', '+'], 4: ['/', '/', '/', '/', 'id', '*']}\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 2.611033552451075e+25\tTest mse: inf\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Final expression:  None\n",
      "Test R2:  0.0\n",
      "Test error:  10000\n",
      "Relative error:  100\n",
      "Reward:  9.999000099990002e-05\n",
      "\n",
      "\n",
      "Operators of each layer obtained by sampling:  {1: ['*', '+'], 2: ['-', '*'], 3: ['/', '-'], 4: ['*', '-'], 5: ['pow2', '/']}\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 1.3402080683714427e+36\tTest mse: inf\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 3490003451641856.0\tTest mse: 2.1218901047340255e+22\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 3.6050314903259277\tTest mse: 2331103163580416.0\n",
      "Epoch: 1000\tTotal training loss: nan\tTest mse: nan\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 1.2456417083740234\tTest mse: 18.129741668701172\n",
      "Epoch: 1000\tTotal training loss: 0.19755712151527405\tTest mse: 6.376043796539307\n",
      "Epoch: 2000\tTotal training loss: nan\tTest mse: nan\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 2193362911232.0\tTest mse: 6.315449104654664e+17\n",
      "Final expression:  None\n",
      "Test R2:  0.0\n",
      "Test error:  10000\n",
      "Relative error:  100\n",
      "Reward:  9.999000099990002e-05\n",
      "\n",
      "\n",
      "Operators of each layer obtained by sampling:  {1: ['pow2', '*', 'id', '/'], 2: ['/', '+', '/', '-']}\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 1.5919489860534668\tTest mse: 39.85441207885742\n",
      "Epoch: 1000\tTotal training loss: 0.1048789769411087\tTest mse: 5.326553821563721\n",
      "Epoch: 2000\tTotal training loss: 0.067667655646801\tTest mse: 3.6145596504211426\n",
      "Epoch: 3000\tTotal training loss: 0.06211519241333008\tTest mse: 3.0688164234161377\n",
      "Epoch: 4000\tTotal training loss: 0.06032497435808182\tTest mse: 3.1651360988616943\n",
      "Epoch: 5000\tTotal training loss: 0.0576930046081543\tTest mse: 3.140998125076294\n",
      "Epoch: 6000\tTotal training loss: 0.05825234204530716\tTest mse: 3.2122745513916016\n",
      "Epoch: 7000\tTotal training loss: 0.05896679311990738\tTest mse: 3.1922738552093506\n",
      "Epoch: 8000\tTotal training loss: 0.059876810759305954\tTest mse: 3.239697217941284\n",
      "Epoch: 9000\tTotal training loss: 0.05657216161489487\tTest mse: 3.215352773666382\n",
      "Epoch: 10000\tTotal training loss: 0.056466247886419296\tTest mse: 3.3592922687530518\n",
      "Epoch: 11000\tTotal training loss: 0.058433160185813904\tTest mse: 3.294537305831909\n",
      "Epoch: 12000\tTotal training loss: 0.05634695664048195\tTest mse: 3.2878262996673584\n",
      "Epoch: 0\tTotal training loss: 0.0586652047932148\tTest mse: 3.293304443359375\n",
      "Epoch: 1000\tTotal training loss: 0.05467051640152931\tTest mse: 2.9086861610412598\n",
      "Epoch: 2000\tTotal training loss: 0.05412232503294945\tTest mse: 2.796266555786133\n",
      "Epoch: 3000\tTotal training loss: 0.053820349276065826\tTest mse: 2.733518362045288\n",
      "Epoch: 4000\tTotal training loss: 0.054046038538217545\tTest mse: 2.7528293132781982\n",
      "Epoch: 5000\tTotal training loss: 0.053533926606178284\tTest mse: 2.713865280151367\n",
      "Epoch: 6000\tTotal training loss: 0.0528225302696228\tTest mse: 2.711954355239868\n",
      "Epoch: 7000\tTotal training loss: 0.05286288261413574\tTest mse: 2.7748608589172363\n",
      "Epoch: 8000\tTotal training loss: 0.05210842937231064\tTest mse: 2.692749261856079\n",
      "Epoch: 9000\tTotal training loss: 0.05221828073263168\tTest mse: 2.9023597240448\n",
      "Epoch: 10000\tTotal training loss: 0.05123833939433098\tTest mse: 3.0395846366882324\n",
      "error_expr_sorted [(3.0395846366882324, 0.8302358984947205, 0.198042730034764*x_1**2 + 2.15631681182499*x_1)]\n",
      "Constructing BFGS loss...\n",
      "Final expression:  1.02825122438158*x_1**2 + 1.56500321290605*x_1\n",
      "Test R2:  0.9761562730694662\n",
      "Test error:  0.018769081764957937\n",
      "Relative error:  0.4818594509435573\n",
      "Reward:  0.9815767065364395\n",
      "\n",
      "\n",
      "Operators of each layer obtained by sampling:  {1: ['*', 'id'], 2: ['pow2', '+'], 3: ['+', '/'], 4: ['pow3', '+'], 5: ['-', 'pow3']}\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 6.384377771658215e+24\tTest mse: 4.430213239170662e+16\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 1.4007696521381993e+25\tTest mse: 7.96092262075092e+35\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 230758192.0\tTest mse: 9846573236224.0\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Final expression:  None\n",
      "Test R2:  0.0\n",
      "Test error:  10000\n",
      "Relative error:  100\n",
      "Reward:  9.999000099990002e-05\n",
      "\n",
      "\n",
      "Operators of each layer obtained by sampling:  {1: ['pow3', '*', '+', '/', '*', '+'], 2: ['pow3', 'pow2', '*', 'id', 'pow3', '/']}\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 18351846.0\tTest mse: 38629711872.0\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 9965796352.0\tTest mse: 36361641984.0\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 212838957056.0\tTest mse: 160804179542016.0\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 1130.69677734375\tTest mse: 35270.4296875\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 3534874624.0\tTest mse: 588982656.0\n",
      "Final expression:  None\n",
      "Test R2:  0.0\n",
      "Test error:  10000\n",
      "Relative error:  100\n",
      "Reward:  9.999000099990002e-05\n",
      "\n",
      "\n",
      "******************** Epoch 02 ********************\n",
      "Operators of each layer obtained by sampling:  {1: ['pow3', '+', 'pow2', 'pow2', '*', 'id'], 2: ['*', 'pow2', 'pow3', 'pow2', '+', '*']}\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 76217312.0\tTest mse: 75532443648.0\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 1138610.25\tTest mse: 4910935552.0\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 161125.96875\tTest mse: 47858876.0\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 316663.96875\tTest mse: 119948568.0\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 3280.705078125\tTest mse: 316218.5\n",
      "Final expression:  None\n",
      "Test R2:  0.0\n",
      "Test error:  10000\n",
      "Relative error:  100\n",
      "Reward:  9.999000099990002e-05\n",
      "\n",
      "\n",
      "Operators of each layer obtained by sampling:  {1: ['+', 'pow3', '-'], 2: ['pow2', 'pow3', '/'], 3: ['id', '*', 'pow3'], 4: ['/', '+', '*']}\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 2.8198404109453582e+22\tTest mse: inf\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 9511257690341376.0\tTest mse: 1.7596253137927143e+19\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 4.668946952322842e+32\tTest mse: inf\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 4725270528.0\tTest mse: 283254569566208.0\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 2.1884549621790747e+35\tTest mse: inf\n",
      "Final expression:  None\n",
      "Test R2:  0.0\n",
      "Test error:  10000\n",
      "Relative error:  100\n",
      "Reward:  9.999000099990002e-05\n",
      "\n",
      "\n",
      "Operators of each layer obtained by sampling:  {1: ['-', '/'], 2: ['*', '/'], 3: ['pow2', '*']}\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 76212904.0\tTest mse: 6471391744.0\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 262935968.0\tTest mse: 1823380096.0\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 1.3934879302978516\tTest mse: 20.766469955444336\n",
      "Epoch: 1000\tTotal training loss: 0.08929862827062607\tTest mse: 5.109737396240234\n",
      "Epoch: 2000\tTotal training loss: 0.08481290936470032\tTest mse: 3.3641092777252197\n",
      "Epoch: 3000\tTotal training loss: 0.07778285443782806\tTest mse: 1.9871907234191895\n",
      "Epoch: 4000\tTotal training loss: 0.07251940667629242\tTest mse: 2.0267796516418457\n",
      "Epoch: 5000\tTotal training loss: 0.067259281873703\tTest mse: 2.1155900955200195\n",
      "Epoch: 6000\tTotal training loss: 0.06055133789777756\tTest mse: 2.133950710296631\n",
      "Epoch: 7000\tTotal training loss: 0.05849968641996384\tTest mse: 2.193249225616455\n",
      "Epoch: 8000\tTotal training loss: 0.0556754395365715\tTest mse: 2.276108503341675\n",
      "Epoch: 9000\tTotal training loss: 0.05329202115535736\tTest mse: 2.313115358352661\n",
      "Epoch: 10000\tTotal training loss: 0.05147786810994148\tTest mse: 2.2868666648864746\n",
      "Epoch: 11000\tTotal training loss: 0.0501156710088253\tTest mse: 2.2920284271240234\n",
      "Epoch: 12000\tTotal training loss: 0.048995520919561386\tTest mse: 2.2702324390411377\n",
      "Epoch: 0\tTotal training loss: 0.051651258021593094\tTest mse: 2.2141504287719727\n",
      "Epoch: 1000\tTotal training loss: 0.04684966057538986\tTest mse: 1.9911530017852783\n",
      "Epoch: 2000\tTotal training loss: 0.045297276228666306\tTest mse: 1.999705195426941\n",
      "Epoch: 3000\tTotal training loss: 0.04375579208135605\tTest mse: 2.0238802433013916\n",
      "Epoch: 4000\tTotal training loss: 0.042566027492284775\tTest mse: 2.03481388092041\n",
      "Epoch: 5000\tTotal training loss: 0.04208378121256828\tTest mse: 2.040910482406616\n",
      "Epoch: 6000\tTotal training loss: 0.04169989377260208\tTest mse: 2.062171220779419\n",
      "Epoch: 7000\tTotal training loss: 0.041387781500816345\tTest mse: 2.072019100189209\n",
      "Epoch: 8000\tTotal training loss: 0.041117094457149506\tTest mse: 2.0815131664276123\n",
      "Epoch: 9000\tTotal training loss: 0.04088506102561951\tTest mse: 2.0889601707458496\n",
      "Epoch: 10000\tTotal training loss: 0.04068988561630249\tTest mse: 2.0936243534088135\n",
      "error_expr_sorted [(2.0936243534088135, 0.8830687999725342, 38516.0810684683*x_1**3)]\n",
      "Constructing BFGS loss...\n",
      "Final expression:  2.08478166416377*x_1**3\n",
      "Test R2:  0.6663727702290954\n",
      "Test error:  0.26262155965926937\n",
      "Relative error:  0.6742206168670881\n",
      "Reward:  0.7920029500128761\n",
      "\n",
      "\n",
      "Operators of each layer obtained by sampling:  {1: ['*', '-', 'pow3', '*'], 2: ['+', 'id', 'pow2', 'id'], 3: ['id', 'id', 'id', '/'], 4: ['pow3', '-', '/', '-'], 5: ['-', '/', '-', 'pow2']}\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 1.2332895910371617e+23\tTest mse: inf\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 3.6369255639994295e+22\tTest mse: 3.419200645486221e+28\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 1.758198679863489e+21\tTest mse: 1.4128196505875934e+33\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Final expression:  None\n",
      "Test R2:  0.0\n",
      "Test error:  10000\n",
      "Relative error:  100\n",
      "Reward:  9.999000099990002e-05\n",
      "\n",
      "\n",
      "Operators of each layer obtained by sampling:  {1: ['*', 'pow2', '/', '-', 'id', 'id'], 2: ['id', '*', '-', '-', '+', 'pow3']}\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 28163.865234375\tTest mse: 11656031.0\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 17648058368.0\tTest mse: 127199496.0\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 155.21710205078125\tTest mse: 12081.1953125\n",
      "Epoch: 1000\tTotal training loss: 0.08708196878433228\tTest mse: 0.012013066560029984\n",
      "Epoch: 2000\tTotal training loss: 0.08800981938838959\tTest mse: 0.5733699798583984\n",
      "Epoch: 3000\tTotal training loss: 0.07882143557071686\tTest mse: 0.025777921080589294\n",
      "Epoch: 4000\tTotal training loss: 0.07846769690513611\tTest mse: 0.050639986991882324\n",
      "Epoch: 5000\tTotal training loss: 0.07829106599092484\tTest mse: 0.05921700969338417\n",
      "Epoch: 6000\tTotal training loss: 0.07818496227264404\tTest mse: 0.06729128211736679\n",
      "Epoch: 7000\tTotal training loss: 0.07811572402715683\tTest mse: 0.07559607177972794\n",
      "Epoch: 8000\tTotal training loss: 0.07809385657310486\tTest mse: 0.06964989006519318\n",
      "Epoch: 9000\tTotal training loss: 0.07807542383670807\tTest mse: 0.07023915648460388\n",
      "Epoch: 10000\tTotal training loss: 0.0780625119805336\tTest mse: 0.07176738232374191\n",
      "Epoch: 11000\tTotal training loss: 0.07805328071117401\tTest mse: 0.07327623665332794\n",
      "Epoch: 12000\tTotal training loss: 0.07804662734270096\tTest mse: 0.07461567968130112\n",
      "Epoch: 0\tTotal training loss: 0.07868283987045288\tTest mse: 0.06809856742620468\n",
      "Epoch: 1000\tTotal training loss: 0.0775635689496994\tTest mse: 0.0363941416144371\n",
      "Epoch: 2000\tTotal training loss: 0.0773576870560646\tTest mse: 0.06299532204866409\n",
      "Epoch: 3000\tTotal training loss: 0.07712733745574951\tTest mse: 0.09449826925992966\n",
      "Epoch: 4000\tTotal training loss: 0.07684707641601562\tTest mse: 0.14132699370384216\n",
      "Epoch: 5000\tTotal training loss: 0.07647302746772766\tTest mse: 0.22074517607688904\n",
      "Epoch: 6000\tTotal training loss: 0.07594823837280273\tTest mse: 0.3499354422092438\n",
      "Epoch: 7000\tTotal training loss: 0.07523757964372635\tTest mse: 0.517556369304657\n",
      "Epoch: 8000\tTotal training loss: 0.07435446977615356\tTest mse: 0.6959754824638367\n",
      "Epoch: 9000\tTotal training loss: 0.07372430711984634\tTest mse: 0.7836164236068726\n",
      "Epoch: 10000\tTotal training loss: 0.07364288717508316\tTest mse: 0.7879424095153809\n",
      "error_expr_sorted [(0.7879424095153809, 0.9559925198554993, 1.0125459229538*x_1**3)]\n",
      "Constructing BFGS loss...\n",
      "Final expression:  2.08478166804615*x_1**3\n",
      "Test R2:  0.6663727702290954\n",
      "Test error:  0.26262155965926937\n",
      "Relative error:  0.6742206168670881\n",
      "Reward:  0.7920029500128761\n",
      "\n",
      "\n",
      "Operators of each layer obtained by sampling:  {1: ['pow2', '+', '+', '+', '+', '/'], 2: ['*', '-', '+', 'pow2', '+', 'pow2']}\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 219736.71875\tTest mse: 6674339.0\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 885057.75\tTest mse: 12606292.0\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 10089.146484375\tTest mse: 267066.65625\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 4575786.0\tTest mse: 1350975.0\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 3385.455078125\tTest mse: 35072.484375\n",
      "Final expression:  None\n",
      "Test R2:  0.0\n",
      "Test error:  10000\n",
      "Relative error:  100\n",
      "Reward:  9.999000099990002e-05\n",
      "\n",
      "\n",
      "Operators of each layer obtained by sampling:  {1: ['*', 'id', 'id', '-', '*'], 2: ['id', '*', 'pow3', '*', '/'], 3: ['+', 'id', 'pow2', '+', '+'], 4: ['/', '*', 'id', '+', 'pow2'], 5: ['-', 'pow3', '*', 'pow2', 'id']}\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Final expression:  None\n",
      "Test R2:  0.0\n",
      "Test error:  10000\n",
      "Relative error:  100\n",
      "Reward:  9.999000099990002e-05\n",
      "\n",
      "\n",
      "Operators of each layer obtained by sampling:  {1: ['/', 'pow3', '/', '-', '/', '/'], 2: ['id', '-', 'pow3', '/', 'id', 'pow2'], 3: ['-', '-', 'id', 'id', '*', '/'], 4: ['pow3', '*', 'pow2', 'pow3', '-', '-']}\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Final expression:  None\n",
      "Test R2:  0.0\n",
      "Test error:  10000\n",
      "Relative error:  100\n",
      "Reward:  9.999000099990002e-05\n",
      "\n",
      "\n",
      "Operators of each layer obtained by sampling:  {1: ['*', '-', '-'], 2: ['id', '+', '*'], 3: ['*', '-', '+'], 4: ['+', '*', 'id']}\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 627431243776.0\tTest mse: 1333475810476032.0\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 778374507134976.0\tTest mse: 4.244369145279283e+17\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 6587504874487808.0\tTest mse: 1.2208013942638772e+19\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 2.7974388661480606e+35\tTest mse: inf\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 40839069696.0\tTest mse: 865370898432.0\n",
      "Final expression:  None\n",
      "Test R2:  0.0\n",
      "Test error:  10000\n",
      "Relative error:  100\n",
      "Reward:  9.999000099990002e-05\n",
      "\n",
      "\n",
      "Operators of each layer obtained by sampling:  {1: ['id', '+'], 2: ['*', 'pow2'], 3: ['-', '-'], 4: ['pow2', 'id'], 5: ['+', '*']}\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 0.9402444958686829\tTest mse: 77.13841247558594\n",
      "Epoch: 1000\tTotal training loss: 0.836092472076416\tTest mse: 17.77520751953125\n",
      "Epoch: 2000\tTotal training loss: 0.8356472849845886\tTest mse: 17.650047302246094\n",
      "Epoch: 3000\tTotal training loss: 0.8354172110557556\tTest mse: 17.8953800201416\n",
      "Epoch: 4000\tTotal training loss: 0.8354165554046631\tTest mse: 17.89693832397461\n",
      "Epoch: 5000\tTotal training loss: 0.8354171514511108\tTest mse: 17.901845932006836\n",
      "Epoch: 6000\tTotal training loss: 0.8354178071022034\tTest mse: 17.905113220214844\n",
      "Epoch: 7000\tTotal training loss: 0.8354182839393616\tTest mse: 17.907424926757812\n",
      "Epoch: 8000\tTotal training loss: 0.8354188799858093\tTest mse: 17.909154891967773\n",
      "Epoch: 9000\tTotal training loss: 0.8354191184043884\tTest mse: 17.910476684570312\n",
      "Epoch: 10000\tTotal training loss: 0.8354194760322571\tTest mse: 17.91150665283203\n",
      "Epoch: 11000\tTotal training loss: 0.8354196548461914\tTest mse: 17.91227149963379\n",
      "Epoch: 12000\tTotal training loss: 0.8354198336601257\tTest mse: 17.912874221801758\n",
      "Epoch: 0\tTotal training loss: 0.8354215025901794\tTest mse: 17.917638778686523\n",
      "Epoch: 1000\tTotal training loss: 0.8354164361953735\tTest mse: 17.937166213989258\n",
      "Epoch: 2000\tTotal training loss: 0.835416316986084\tTest mse: 17.937166213989258\n",
      "Epoch: 3000\tTotal training loss: 0.8354163765907288\tTest mse: 17.937175750732422\n",
      "Epoch: 4000\tTotal training loss: 0.8354163765907288\tTest mse: 17.937162399291992\n",
      "Epoch: 5000\tTotal training loss: 0.8354164361953735\tTest mse: 17.93718719482422\n",
      "Epoch: 6000\tTotal training loss: 0.8354163765907288\tTest mse: 17.93719482421875\n",
      "Epoch: 7000\tTotal training loss: 0.8354163765907288\tTest mse: 17.93719482421875\n",
      "Epoch: 8000\tTotal training loss: 0.8354163765907288\tTest mse: 17.93719482421875\n",
      "Epoch: 9000\tTotal training loss: 0.8354163765907288\tTest mse: 17.93719482421875\n",
      "Epoch: 10000\tTotal training loss: 0.8354163765907288\tTest mse: 17.93719482421875\n",
      "error_expr_sorted [(17.93719482421875, -0.0018121004104614258, 0.422505408745836*x_1**2)]\n",
      "Constructing BFGS loss...\n",
      "Final expression:  0.444971503161891*x_1**2\n",
      "Test R2:  0.0\n",
      "Test error:  0.7913387738488297\n",
      "Relative error:  1.1417468597390135\n",
      "Reward:  0.5582416986662009\n",
      "\n",
      "\n",
      "******************** Epoch 03 ********************\n",
      "Operators of each layer obtained by sampling:  {1: ['*', '/'], 2: ['pow2', 'id'], 3: ['*', '*']}\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 3771050005364736.0\tTest mse: 4.62318912625705e+16\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 11663.671875\tTest mse: 21310.599609375\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 1.2182292938232422\tTest mse: 570.308837890625\n",
      "Epoch: 1000\tTotal training loss: 0.8203914761543274\tTest mse: 279894.625\n",
      "Epoch: 2000\tTotal training loss: 0.8143772482872009\tTest mse: 510846.15625\n",
      "Epoch: 3000\tTotal training loss: 0.8015543818473816\tTest mse: 441378.84375\n",
      "Epoch: 4000\tTotal training loss: 0.7995560169219971\tTest mse: 478133.71875\n",
      "Epoch: 5000\tTotal training loss: 0.7996312379837036\tTest mse: 454360.8125\n",
      "Epoch: 6000\tTotal training loss: 0.7981445789337158\tTest mse: 467874.625\n",
      "Epoch: 7000\tTotal training loss: 0.7977117300033569\tTest mse: 471479.75\n",
      "Epoch: 8000\tTotal training loss: 0.7976999878883362\tTest mse: 490342.875\n",
      "Epoch: 9000\tTotal training loss: 0.7980424761772156\tTest mse: 558408.8125\n",
      "Epoch: 10000\tTotal training loss: 0.7972886562347412\tTest mse: 462469.84375\n",
      "Epoch: 11000\tTotal training loss: 0.7981043457984924\tTest mse: 514911.59375\n",
      "Epoch: 12000\tTotal training loss: 0.796941339969635\tTest mse: 553051.8125\n",
      "Epoch: 0\tTotal training loss: 0.7973860502243042\tTest mse: 545586.5\n",
      "Epoch: 1000\tTotal training loss: 0.7961068153381348\tTest mse: 437301.125\n",
      "Epoch: 2000\tTotal training loss: 0.795876145362854\tTest mse: 437410.625\n",
      "Epoch: 3000\tTotal training loss: 0.7957426905632019\tTest mse: 439489.75\n",
      "Epoch: 4000\tTotal training loss: 0.7956425547599792\tTest mse: 439194.90625\n",
      "Epoch: 5000\tTotal training loss: 0.7955588698387146\tTest mse: 438897.1875\n",
      "Epoch: 6000\tTotal training loss: 0.7955246567726135\tTest mse: 438235.625\n",
      "Epoch: 7000\tTotal training loss: 0.7954874634742737\tTest mse: 437896.3125\n",
      "Epoch: 8000\tTotal training loss: 0.7954540252685547\tTest mse: 436761.90625\n",
      "Epoch: 9000\tTotal training loss: 0.7953914403915405\tTest mse: 437329.0\n",
      "Epoch: 10000\tTotal training loss: 0.7953712940216064\tTest mse: 436831.09375\n",
      "error_expr_sorted [(436831.09375, -24396.498046875, -3.06736692142001*x_1**2*(4.3824476502213*x_1**4 - 3.09273276567086*x_1**2))]\n",
      "Constructing BFGS loss...\n",
      "Final expression:  -0.59592231625295*x_1**2*(2.68328607587902*x_1**4 - 2.95245783792611*x_1**2)\n",
      "Test R2:  0.0\n",
      "Test error:  0.7971896988426497\n",
      "Relative error:  1.1251428292126868\n",
      "Reward:  0.5564242887904253\n",
      "\n",
      "\n",
      "Operators of each layer obtained by sampling:  {1: ['pow2', '/'], 2: ['*', 'pow2'], 3: ['-', 'pow3'], 4: ['pow3', '*'], 5: ['id', '/']}\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 77738857725952.0\tTest mse: 1580387794944.0\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 8260725768192.0\tTest mse: 1.5395486583221328e+26\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 2.2504601687179657e+20\tTest mse: 2.5579367482459895e+20\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 2.7607465663135744e+17\tTest mse: 2341225596190720.0\n",
      "Final expression:  None\n",
      "Test R2:  0.0\n",
      "Test error:  10000\n",
      "Relative error:  100\n",
      "Reward:  9.999000099990002e-05\n",
      "\n",
      "\n",
      "Operators of each layer obtained by sampling:  {1: ['/', '+', '-'], 2: ['+', 'id', '/'], 3: ['-', 'pow2', '/']}\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 536.513671875\tTest mse: 5377.94384765625\n",
      "Epoch: 1000\tTotal training loss: 0.11147105693817139\tTest mse: 6.430440902709961\n",
      "Epoch: 2000\tTotal training loss: 0.09303941577672958\tTest mse: 2.0772342681884766\n",
      "Epoch: 3000\tTotal training loss: 0.07750058174133301\tTest mse: 6485.89892578125\n",
      "Epoch: 4000\tTotal training loss: 0.07769177854061127\tTest mse: 11355537.0\n",
      "Epoch: 5000\tTotal training loss: 0.07342226803302765\tTest mse: 572319808.0\n",
      "Epoch: 6000\tTotal training loss: 0.06730183959007263\tTest mse: 60384.28515625\n",
      "Epoch: 7000\tTotal training loss: 0.06551098078489304\tTest mse: 631481.8125\n",
      "Epoch: 8000\tTotal training loss: 0.061480939388275146\tTest mse: 41997750272.0\n",
      "Epoch: 9000\tTotal training loss: 0.05993526056408882\tTest mse: 514545.53125\n",
      "Epoch: 10000\tTotal training loss: 0.06003015860915184\tTest mse: 3533166.0\n",
      "Epoch: 11000\tTotal training loss: 0.059753429144620895\tTest mse: 110670.953125\n",
      "Epoch: 12000\tTotal training loss: 0.059611573815345764\tTest mse: 30783.005859375\n",
      "Epoch: 0\tTotal training loss: 0.05623349919915199\tTest mse: 226154.9375\n",
      "Epoch: 1000\tTotal training loss: 0.055533889681100845\tTest mse: 13460507.0\n",
      "Epoch: 2000\tTotal training loss: 0.055145133286714554\tTest mse: 2968286.75\n",
      "Epoch: 3000\tTotal training loss: 0.05478326603770256\tTest mse: 212415024.0\n",
      "Epoch: 4000\tTotal training loss: 0.05441528558731079\tTest mse: 4053318400.0\n",
      "Epoch: 5000\tTotal training loss: 0.05415491759777069\tTest mse: 200196704.0\n",
      "Epoch: 6000\tTotal training loss: 0.05408652871847153\tTest mse: 15715821.0\n",
      "Epoch: 7000\tTotal training loss: 0.05375867336988449\tTest mse: 13371933.0\n",
      "Epoch: 8000\tTotal training loss: 0.05330151319503784\tTest mse: 35441.99609375\n",
      "Epoch: 9000\tTotal training loss: 0.053146425634622574\tTest mse: 1812621.625\n",
      "Epoch: 10000\tTotal training loss: 0.053016047924757004\tTest mse: 14798.8125\n",
      "error_expr_sorted [(14798.8125, -825.5299682617188, 3.01231679906505*x_1**2 + 0.977977926739994*x_1)]\n",
      "Constructing BFGS loss...\n",
      "Final expression:  1.02824839704622*x_1**2 + 1.56500061908921*x_1\n",
      "Test R2:  0.9761562757908044\n",
      "Test error:  0.01876907962280037\n",
      "Relative error:  0.4818581455072909\n",
      "Reward:  0.9815767086003929\n",
      "\n",
      "\n",
      "Operators of each layer obtained by sampling:  {1: ['/', 'id', '-', 'pow3', '*', 'pow3'], 2: ['pow3', 'pow3', '/', 'id', '+', 'id'], 3: ['+', '-', 'id', '+', '*', '*'], 4: ['-', '+', '-', 'pow3', '*', '/'], 5: ['-', 'pow2', '/', '+', '+', 'id']}\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Final expression:  None\n",
      "Test R2:  0.0\n",
      "Test error:  10000\n",
      "Relative error:  100\n",
      "Reward:  9.999000099990002e-05\n",
      "\n",
      "\n",
      "Operators of each layer obtained by sampling:  {1: ['+', '/', 'pow2'], 2: ['-', '/', 'pow3']}\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 2719.71630859375\tTest mse: 12685016.0\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 6476.5224609375\tTest mse: 341437.125\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 4516.1181640625\tTest mse: 11315227.0\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 1372717.625\tTest mse: 17406632.0\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 1.290334939956665\tTest mse: 17.200977325439453\n",
      "Epoch: 1000\tTotal training loss: 0.06431587040424347\tTest mse: 1.8441741466522217\n",
      "Epoch: 2000\tTotal training loss: 0.047411125153303146\tTest mse: 0.024254001677036285\n",
      "Epoch: 3000\tTotal training loss: 0.04463106766343117\tTest mse: 0.005714991129934788\n",
      "Epoch: 4000\tTotal training loss: 0.042504437267780304\tTest mse: 0.003836918156594038\n",
      "Epoch: 5000\tTotal training loss: 0.040234290063381195\tTest mse: 0.0016730503411963582\n",
      "Epoch: 6000\tTotal training loss: 0.03863484784960747\tTest mse: 0.002564282389357686\n",
      "Epoch: 7000\tTotal training loss: 0.036806873977184296\tTest mse: 0.0029152578208595514\n",
      "Epoch: 8000\tTotal training loss: 0.03683217614889145\tTest mse: 0.027154941111803055\n",
      "Epoch: 9000\tTotal training loss: 0.03704006224870682\tTest mse: 0.01936078630387783\n",
      "Epoch: 10000\tTotal training loss: 0.03602312505245209\tTest mse: 0.009600263088941574\n",
      "Epoch: 11000\tTotal training loss: 0.03646232932806015\tTest mse: 0.028158163651823997\n",
      "Epoch: 12000\tTotal training loss: 0.0375325009226799\tTest mse: 0.0019240179099142551\n",
      "Epoch: 0\tTotal training loss: 0.03605376556515694\tTest mse: 0.0027116842102259398\n",
      "Epoch: 1000\tTotal training loss: 0.03532279282808304\tTest mse: 0.0023671744856983423\n",
      "Epoch: 2000\tTotal training loss: 0.03516203537583351\tTest mse: 0.004826505668461323\n",
      "Epoch: 3000\tTotal training loss: 0.03507758677005768\tTest mse: 0.003100968897342682\n",
      "Epoch: 4000\tTotal training loss: 0.03484925255179405\tTest mse: 0.0016514568123966455\n",
      "Epoch: 5000\tTotal training loss: 0.03491900488734245\tTest mse: 0.0014839789364486933\n",
      "Epoch: 6000\tTotal training loss: 0.034819357097148895\tTest mse: 0.005342273041605949\n",
      "Epoch: 7000\tTotal training loss: 0.03461971879005432\tTest mse: 0.002555775921791792\n",
      "Epoch: 8000\tTotal training loss: 0.03486746922135353\tTest mse: 0.0030028768815100193\n",
      "Epoch: 9000\tTotal training loss: 0.034666452556848526\tTest mse: 0.008735342882573605\n",
      "Epoch: 10000\tTotal training loss: 0.0347801148891449\tTest mse: 0.008774655871093273\n",
      "error_expr_sorted [(0.008774655871093273, 0.9995099306106567, 0.990187355015626*x_1**3 + 0.502849104460033*x_1)]\n",
      "Constructing BFGS loss...\n",
      "Final expression:  0.442117103897699*x_1**3 + 1.09182169941824*x_1\n",
      "Test R2:  0.7377193441005452\n",
      "Test error:  0.2064596315117021\n",
      "Relative error:  0.694994526926964\n",
      "Reward:  0.8288714963027757\n",
      "\n",
      "\n",
      "Operators of each layer obtained by sampling:  {1: ['+', '/'], 2: ['-', 'id'], 3: ['*', '/']}\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 153.9041748046875\tTest mse: 391.83941650390625\n",
      "Epoch: 1000\tTotal training loss: 0.07556389272212982\tTest mse: 5.08192253112793\n",
      "Epoch: 2000\tTotal training loss: 0.07610023766756058\tTest mse: 4.633307933807373\n",
      "Epoch: 3000\tTotal training loss: 0.06522540003061295\tTest mse: 5.388952255249023\n",
      "Epoch: 4000\tTotal training loss: 0.06411521136760712\tTest mse: 5.2349677085876465\n",
      "Epoch: 5000\tTotal training loss: 0.062126435339450836\tTest mse: 904.748779296875\n",
      "Epoch: 6000\tTotal training loss: 0.053829044103622437\tTest mse: 25516.33203125\n",
      "Epoch: 7000\tTotal training loss: 0.05149625241756439\tTest mse: 104.47976684570312\n",
      "Epoch: 8000\tTotal training loss: 0.0491340234875679\tTest mse: 63.33253479003906\n",
      "Epoch: 9000\tTotal training loss: 0.0489153116941452\tTest mse: 152.35914611816406\n",
      "Epoch: 10000\tTotal training loss: 0.048670947551727295\tTest mse: 317.16229248046875\n",
      "Epoch: 11000\tTotal training loss: 0.04843322932720184\tTest mse: 84.41706848144531\n",
      "Epoch: 12000\tTotal training loss: 0.048225779086351395\tTest mse: 1031.36669921875\n",
      "Epoch: 0\tTotal training loss: 0.05429747700691223\tTest mse: 115.3699722290039\n",
      "Epoch: 1000\tTotal training loss: 0.04633621498942375\tTest mse: 568.91845703125\n",
      "Epoch: 2000\tTotal training loss: 0.04577025771141052\tTest mse: 5020.453125\n",
      "Epoch: 3000\tTotal training loss: 0.045279018580913544\tTest mse: 541.5260620117188\n",
      "Epoch: 4000\tTotal training loss: 0.04526824876666069\tTest mse: 391.76544189453125\n",
      "Epoch: 5000\tTotal training loss: 0.04561813920736313\tTest mse: 193.13290405273438\n",
      "Epoch: 6000\tTotal training loss: 0.0457037091255188\tTest mse: 185.61044311523438\n",
      "Epoch: 7000\tTotal training loss: 0.04570126533508301\tTest mse: 186.6182403564453\n",
      "Epoch: 8000\tTotal training loss: 0.04566976800560951\tTest mse: 192.82008361816406\n",
      "Epoch: 9000\tTotal training loss: 0.04552934318780899\tTest mse: 208.52908325195312\n",
      "Epoch: 10000\tTotal training loss: 0.04585544392466545\tTest mse: 187.01348876953125\n",
      "error_expr_sorted [(187.01348876953125, -9.444910049438477, 2.00696303685574*x_1**2 + 11355.3772865039*x_1)]\n",
      "Constructing BFGS loss...\n",
      "Final expression:  1.02824907621994*x_1**2 + 1.56500216957606*x_1\n",
      "Test R2:  0.9761562780695305\n",
      "Test error:  0.018769077829054067\n",
      "Relative error:  0.4818590329980543\n",
      "Reward:  0.9815767103286546\n",
      "\n",
      "\n",
      "Operators of each layer obtained by sampling:  {1: ['pow2', '*', 'pow3'], 2: ['id', '*', '/'], 3: ['pow3', '+', 'pow3']}\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 1182539251187712.0\tTest mse: 1.0413339071132462e+24\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 203830149120.0\tTest mse: 2.8079912065695744e+17\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 133188788224.0\tTest mse: 1.4501638299035317e+20\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 6.304982552819404e+23\tTest mse: 1.4200041468267615e+31\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 117090816.0\tTest mse: 9.150371989553152e+16\n",
      "Final expression:  None\n",
      "Test R2:  0.0\n",
      "Test error:  10000\n",
      "Relative error:  100\n",
      "Reward:  9.999000099990002e-05\n",
      "\n",
      "\n",
      "Operators of each layer obtained by sampling:  {1: ['/', '-'], 2: ['pow2', '/'], 3: ['id', 'pow3'], 4: ['-', 'pow3'], 5: ['-', 'id']}\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 0.9918473958969116\tTest mse: 21.179311752319336\n",
      "Epoch: 1000\tTotal training loss: 0.0952693447470665\tTest mse: 316383.125\n",
      "Epoch: 2000\tTotal training loss: 0.08053317666053772\tTest mse: 1937650.125\n",
      "Epoch: 3000\tTotal training loss: 0.07377515733242035\tTest mse: 6312271.5\n",
      "Epoch: 4000\tTotal training loss: 0.07090925425291061\tTest mse: 8177502.0\n",
      "Epoch: 5000\tTotal training loss: 0.06900456547737122\tTest mse: 10012534.0\n",
      "Epoch: 6000\tTotal training loss: 0.0667109489440918\tTest mse: 9177106.0\n",
      "Epoch: 7000\tTotal training loss: 0.06419846415519714\tTest mse: 5166007.5\n",
      "Epoch: 8000\tTotal training loss: 0.062261153012514114\tTest mse: 4137355.25\n",
      "Epoch: 9000\tTotal training loss: 0.060731399804353714\tTest mse: 3634706.0\n",
      "Epoch: 10000\tTotal training loss: 0.059513043612241745\tTest mse: 3428678.75\n",
      "Epoch: 11000\tTotal training loss: 0.05854681506752968\tTest mse: 3563203.75\n",
      "Epoch: 12000\tTotal training loss: 0.05833858251571655\tTest mse: 4995867.0\n",
      "Epoch: 0\tTotal training loss: 0.06408776342868805\tTest mse: 6800873.0\n",
      "Epoch: 1000\tTotal training loss: 0.057125840336084366\tTest mse: 111836560.0\n",
      "Epoch: 2000\tTotal training loss: 0.056655801832675934\tTest mse: 118867040.0\n",
      "Epoch: 3000\tTotal training loss: 0.05625911056995392\tTest mse: 105546352.0\n",
      "Epoch: 4000\tTotal training loss: 0.055886153131723404\tTest mse: 90345768.0\n",
      "Epoch: 5000\tTotal training loss: 0.05552421882748604\tTest mse: 77716016.0\n",
      "Epoch: 6000\tTotal training loss: 0.05516932159662247\tTest mse: 68100896.0\n",
      "Epoch: 7000\tTotal training loss: 0.0548177994787693\tTest mse: 60955104.0\n",
      "Epoch: 8000\tTotal training loss: 0.054463889449834824\tTest mse: 55623472.0\n",
      "Epoch: 9000\tTotal training loss: 0.05410030111670494\tTest mse: 51621088.0\n",
      "Epoch: 10000\tTotal training loss: 0.05371744558215141\tTest mse: 48572020.0\n",
      "error_expr_sorted [(48572020.0, -2712800.0, 17993.6227827995*x_1)]\n",
      "Constructing BFGS loss...\n",
      "Final expression:  1.34529596793163*x_1\n",
      "Test R2:  0.7332608209085808\n",
      "Test error:  0.2099692500618869\n",
      "Relative error:  0.7482151499620765\n",
      "Reward:  0.8264672841469752\n",
      "\n",
      "\n",
      "Operators of each layer obtained by sampling:  {1: ['*', 'pow3', '-'], 2: ['+', '/', '-'], 3: ['pow2', 'pow2', '/']}\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 2.2634339332580566\tTest mse: 83.20831298828125\n",
      "Epoch: 1000\tTotal training loss: 0.1293528974056244\tTest mse: 50.814884185791016\n",
      "Epoch: 2000\tTotal training loss: 0.08549462258815765\tTest mse: 1.207703709602356\n",
      "Epoch: 3000\tTotal training loss: 0.07187917083501816\tTest mse: 61.415164947509766\n",
      "Epoch: 4000\tTotal training loss: 0.06639066338539124\tTest mse: 245.3804931640625\n",
      "Epoch: 5000\tTotal training loss: 0.0625850111246109\tTest mse: 2.0966134071350098\n",
      "Epoch: 6000\tTotal training loss: 0.06039866805076599\tTest mse: 0.223169207572937\n",
      "Epoch: 7000\tTotal training loss: 0.05956828594207764\tTest mse: 0.12472730875015259\n",
      "Epoch: 8000\tTotal training loss: 0.05880589410662651\tTest mse: 0.09956880658864975\n",
      "Epoch: 9000\tTotal training loss: 0.058209676295518875\tTest mse: 0.25325241684913635\n",
      "Epoch: 10000\tTotal training loss: 0.057674065232276917\tTest mse: 0.3718143701553345\n",
      "Epoch: 11000\tTotal training loss: 0.057307422161102295\tTest mse: 0.4939529001712799\n",
      "Epoch: 12000\tTotal training loss: 0.056888677179813385\tTest mse: 0.5386947393417358\n",
      "Epoch: 0\tTotal training loss: 0.0668373703956604\tTest mse: 0.47402748465538025\n",
      "Epoch: 1000\tTotal training loss: 0.056537892669439316\tTest mse: 3.038820266723633\n",
      "Epoch: 2000\tTotal training loss: 0.05626741796731949\tTest mse: 19.969470977783203\n",
      "Epoch: 3000\tTotal training loss: 0.05606953427195549\tTest mse: 48.66395568847656\n",
      "Epoch: 4000\tTotal training loss: 0.05593550577759743\tTest mse: 65.17863464355469\n",
      "Epoch: 5000\tTotal training loss: 0.05586528033018112\tTest mse: 48.74839782714844\n",
      "Epoch: 6000\tTotal training loss: 0.055892813950777054\tTest mse: 32.35032653808594\n",
      "Epoch: 7000\tTotal training loss: 0.055962324142456055\tTest mse: 21.38345718383789\n",
      "Epoch: 8000\tTotal training loss: 0.05611403286457062\tTest mse: 15.096334457397461\n",
      "Epoch: 9000\tTotal training loss: 0.05631810426712036\tTest mse: 11.103240966796875\n",
      "Epoch: 10000\tTotal training loss: 0.05646839365363121\tTest mse: 8.103999137878418\n",
      "error_expr_sorted [(8.103999137878418, 0.5473827123641968, 643.965530697787*x_1**2 + 4144.3890059513*x_1 + 1.64612973508621*(0.15538250144305*x_1**2 + x_1)**2)]\n",
      "Constructing BFGS loss...\n",
      "Final expression:  1.31678627498648*x_1**2 + 1.55598921945722*x_1 - 0.43547300966655*(-x_1**2 + 0.0105233348173481*x_1)**2\n",
      "Test R2:  0.9779624060150816\n",
      "Test error:  0.017347346939970157\n",
      "Relative error:  0.4463805261596257\n",
      "Reward:  0.9829484521759964\n",
      "\n",
      "\n",
      "Operators of each layer obtained by sampling:  {1: ['pow2', '+', '+'], 2: ['pow2', 'id', 'id'], 3: ['/', 'pow3', '-']}\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 19632.76171875\tTest mse: 500680.8125\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 44535932.0\tTest mse: 29804061523968.0\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 64.85851287841797\tTest mse: 11215.2666015625\n",
      "Epoch: 1000\tTotal training loss: 0.06601150333881378\tTest mse: 0.04263298958539963\n",
      "Epoch: 2000\tTotal training loss: 0.060677215456962585\tTest mse: 0.14922484755516052\n",
      "Epoch: 3000\tTotal training loss: 0.054921962320804596\tTest mse: 0.011384098790585995\n",
      "Epoch: 4000\tTotal training loss: 0.054598573595285416\tTest mse: 0.013164461590349674\n",
      "Epoch: 5000\tTotal training loss: 0.05379817634820938\tTest mse: 0.04653242975473404\n",
      "Epoch: 6000\tTotal training loss: 0.05409519374370575\tTest mse: 0.016744330525398254\n",
      "Epoch: 7000\tTotal training loss: 0.05338776856660843\tTest mse: 0.01112975925207138\n",
      "Epoch: 8000\tTotal training loss: 0.05401662364602089\tTest mse: 0.01600028946995735\n",
      "Epoch: 9000\tTotal training loss: 0.05401303246617317\tTest mse: 0.01057339459657669\n",
      "Epoch: 10000\tTotal training loss: 0.05343635380268097\tTest mse: 0.03521379828453064\n",
      "Epoch: 11000\tTotal training loss: 0.053168170154094696\tTest mse: 0.04076484590768814\n",
      "Epoch: 12000\tTotal training loss: 0.053109582513570786\tTest mse: 0.03990669548511505\n",
      "Epoch: 0\tTotal training loss: 0.05368480458855629\tTest mse: 0.035023029893636703\n",
      "Epoch: 1000\tTotal training loss: 0.05293554812669754\tTest mse: 124.15746307373047\n",
      "Epoch: 2000\tTotal training loss: 0.05292539671063423\tTest mse: 2.7049126625061035\n",
      "Epoch: 3000\tTotal training loss: 0.05293739587068558\tTest mse: 18.684141159057617\n",
      "Epoch: 4000\tTotal training loss: 0.052962690591812134\tTest mse: 384.3598327636719\n",
      "Epoch: 5000\tTotal training loss: 0.05299974977970123\tTest mse: 12.234260559082031\n",
      "Epoch: 6000\tTotal training loss: 0.05304933711886406\tTest mse: 5.158975601196289\n",
      "Epoch: 7000\tTotal training loss: 0.053113698959350586\tTest mse: 3.738619327545166\n",
      "Epoch: 8000\tTotal training loss: 0.0531967394053936\tTest mse: 3.331880807876587\n",
      "Epoch: 9000\tTotal training loss: 0.05330435186624527\tTest mse: 3.342179298400879\n",
      "Epoch: 10000\tTotal training loss: 0.05345027893781662\tTest mse: 3.6905558109283447\n",
      "error_expr_sorted [(3.6905558109283447, 0.7938783764839172, 0.838632942473404*x_1**3 + 0.650315917491487*x_1**2 + 0.496424325615028*x_1)]\n",
      "Constructing BFGS loss...\n",
      "Final expression:  1.01361844606194*x_1**3 + 1.08143421123085*x_1**2 + 0.995262705571802*x_1\n",
      "Test R2:  0.9989426225728251\n",
      "Test error:  0.000832336465053745\n",
      "Relative error:  0.21217516355860716\n",
      "Reward:  0.9991683557427875\n",
      "\n",
      "\n",
      "~ Early Stopping Met ~\n",
      "Best expression:  1.01361844606194*x_1**3 + 1.08143421123085*x_1**2 + 0.995262705571802*x_1\n",
      "Best reward:      0.9991683557427875\n",
      "mse error:       0.000832336465053745\n",
      "Relative error:   0.21217516355860716\n",
      "Expression:  1.01361844606194*x_1**3 + 1.08143421123085*x_1**2 + 0.995262705571802*x_1\n",
      "R2:  0.9989426225728251\n",
      "Error:  0.000832336465053745\n",
      "Relative Error:  0.21217516355860716\n",
      "log(1 + MSE):  0.0008319902651481184\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Example 1: Input Ground Truth Expression, auto-generate data and extract expression\"\"\"\n",
    "import numpy as np\n",
    "# Define the hyperparameters for the symbolic neural network\n",
    "config = Params()\n",
    "# Define the functions available for symbolic regression\n",
    "funcs = [Identity(),\n",
    "         Square(),\n",
    "         Pow(3),\n",
    "         Sub(),\n",
    "         Plus(),\n",
    "         Product(),\n",
    "         Div()]\n",
    "config.funcs_avail = funcs\n",
    "config.use_gpu = False\n",
    "config.NOISE = 0.05\n",
    "\n",
    "SR = SymbolicRegression.SymboliRegression(config=config, func=\"x_1**3 + x_1**2 + x_1\", func_name=\"Nguyen-1\")\n",
    "eq, R2, error, relative_error = SR.solve_environment()\n",
    "\n",
    "print('Expression: ', eq)\n",
    "print('R2: ', R2)\n",
    "print('Error: ', error)\n",
    "print('Relative Error: ', relative_error)\n",
    "print('log(1 + MSE): ', np.log(1+error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use Device: gpu\n",
      "************************* Start Sampling... *************************\n",
      "\n",
      "******************** Epoch 00 ********************\n",
      "Operators of each layer obtained by sampling:  {1: ['+', '-', 'id'], 2: ['/', 'pow3', '-']}\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 136.77871704101562\tTest mse: 3080.578125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/shared/DySymNetPP/DySymNetPP/SymbolicRegression.py:27: SymPyDeprecationWarning: \n",
      "\n",
      "Passing the function arguments to lambdify() as a set is deprecated. This\n",
      "leads to unpredictable results since sets are unordered. Instead, use a list\n",
      "or tuple for the function arguments.\n",
      "\n",
      "See https://docs.sympy.org/latest/explanation/active-deprecations.html#deprecated-lambdify-arguments-set\n",
      "for details.\n",
      "\n",
      "This has been deprecated since SymPy version 1.6.3. It\n",
      "will be removed in a future version of SymPy.\n",
      "\n",
      "  sp_expr = sp.lambdify(free_symbols, func)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1000\tTotal training loss: 0.14262054860591888\tTest mse: 2.98655366897583\n",
      "Epoch: 2000\tTotal training loss: 0.13662169873714447\tTest mse: 2.993612766265869\n",
      "Epoch: 3000\tTotal training loss: 0.13403010368347168\tTest mse: 2.7155308723449707\n",
      "Epoch: 4000\tTotal training loss: 0.13396522402763367\tTest mse: 2.7129828929901123\n",
      "Epoch: 5000\tTotal training loss: 0.13392995297908783\tTest mse: 2.7110960483551025\n",
      "Epoch: 6000\tTotal training loss: 0.13390371203422546\tTest mse: 2.7098450660705566\n",
      "Epoch: 7000\tTotal training loss: 0.13388320803642273\tTest mse: 2.7090249061584473\n",
      "Epoch: 8000\tTotal training loss: 0.13386660814285278\tTest mse: 2.7084860801696777\n",
      "Epoch: 9000\tTotal training loss: 0.13385289907455444\tTest mse: 2.708131790161133\n",
      "Epoch: 10000\tTotal training loss: 0.13384133577346802\tTest mse: 2.707897901535034\n",
      "Epoch: 11000\tTotal training loss: 0.13383154571056366\tTest mse: 2.7077419757843018\n",
      "Epoch: 12000\tTotal training loss: 0.1338231861591339\tTest mse: 2.7076377868652344\n",
      "Epoch: 0\tTotal training loss: 0.13340285420417786\tTest mse: 2.705763339996338\n",
      "Epoch: 1000\tTotal training loss: 0.13247600197792053\tTest mse: 2.700080633163452\n",
      "Epoch: 2000\tTotal training loss: 0.13235941529273987\tTest mse: 2.675586462020874\n",
      "Epoch: 3000\tTotal training loss: 0.13247263431549072\tTest mse: 2.678567409515381\n",
      "Epoch: 4000\tTotal training loss: 0.1322680115699768\tTest mse: 2.663670778274536\n",
      "Epoch: 5000\tTotal training loss: 0.132175013422966\tTest mse: 2.65539813041687\n",
      "Epoch: 6000\tTotal training loss: 0.13217303156852722\tTest mse: 2.6552789211273193\n",
      "Epoch: 7000\tTotal training loss: 0.13217179477214813\tTest mse: 2.65525221824646\n",
      "Epoch: 8000\tTotal training loss: 0.132170632481575\tTest mse: 2.6552281379699707\n",
      "Epoch: 9000\tTotal training loss: 0.13216951489448547\tTest mse: 2.655207872390747\n",
      "Epoch: 10000\tTotal training loss: 0.1321684867143631\tTest mse: 2.655191421508789\n",
      "error_expr_sorted [(2.655191421508789, 0.8451532125473022, 0.78524*x_1**3 + 1.04479511650418*x_1)]\n",
      "Constructing BFGS loss...\n",
      "Final expression:  0.61623811228571*x_1**3 + 1.1283072011016*x_1\n",
      "Test R2:  0.8061416627089729\n",
      "Test error:  0.1843827330904938\n",
      "Relative error:  0.4384595793108033\n",
      "Reward:  0.8443216639866313\n",
      "\n",
      "\n",
      "Operators of each layer obtained by sampling:  {1: ['*', 'pow3', '/'], 2: ['*', '/', '-'], 3: ['/', 'pow3', '/']}\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 34128472834048.0\tTest mse: 1.6221929828991508e+23\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 1.932868961823705e+21\tTest mse: 2.2645041213776255e+29\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 2.8454141156130816e+16\tTest mse: 3.940007969368935e+26\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 2.099445710186891e+26\tTest mse: 1.2827879956289192e+32\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 2.725810051633434e+19\tTest mse: 4.594448754736418e+19\n",
      "Final expression:  None\n",
      "Test R2:  0.0\n",
      "Test error:  10000\n",
      "Relative error:  100\n",
      "Reward:  9.999000099990002e-05\n",
      "\n",
      "\n",
      "Operators of each layer obtained by sampling:  {1: ['/', 'id', '/', 'pow2', 'pow3'], 2: ['pow3', '*', 'id', 'id', '-'], 3: ['pow3', '-', 'id', '-', 'id'], 4: ['+', '/', 'pow3', '-', 'pow3']}\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: inf\tTest mse: nan\n",
      "Final expression:  None\n",
      "Test R2:  0.0\n",
      "Test error:  10000\n",
      "Relative error:  100\n",
      "Reward:  9.999000099990002e-05\n",
      "\n",
      "\n",
      "Operators of each layer obtained by sampling:  {1: ['pow2', '+', '*'], 2: ['-', '/', '*'], 3: ['-', '+', '-'], 4: ['+', '/', '+'], 5: ['+', '*', '/']}\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 13311963136.0\tTest mse: 76250718208.0\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 253417840.0\tTest mse: 41914585841664.0\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 44533098217472.0\tTest mse: 1.4032544637281894e+17\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 85944393728.0\tTest mse: 2898188828672.0\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 1309268608.0\tTest mse: 4552389632.0\n",
      "Final expression:  None\n",
      "Test R2:  0.0\n",
      "Test error:  10000\n",
      "Relative error:  100\n",
      "Reward:  9.999000099990002e-05\n",
      "\n",
      "\n",
      "Operators of each layer obtained by sampling:  {1: ['pow3', 'id', 'pow2'], 2: ['pow2', 'id', '+']}\n",
      "Training on function Nguyen-1 Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 514.67822265625\tTest mse: 141677.03125\n",
      "Epoch: 1000\tTotal training loss: 0.04827588051557541\tTest mse: 0.022254634648561478\n",
      "Epoch: 2000\tTotal training loss: 0.04788672924041748\tTest mse: 0.024930791929364204\n",
      "Epoch: 3000\tTotal training loss: 0.04466469958424568\tTest mse: 0.020562078803777695\n",
      "Epoch: 4000\tTotal training loss: 0.04050830006599426\tTest mse: 0.015576447360217571\n",
      "Epoch: 5000\tTotal training loss: 0.04027988389134407\tTest mse: 0.014700023457407951\n",
      "Epoch: 6000\tTotal training loss: 0.04017191007733345\tTest mse: 0.014752130024135113\n",
      "Epoch: 7000\tTotal training loss: 0.040122177451848984\tTest mse: 0.014936153776943684\n",
      "Epoch: 8000\tTotal training loss: 0.04009963944554329\tTest mse: 0.015169208869338036\n",
      "Epoch: 9000\tTotal training loss: 0.0400894396007061\tTest mse: 0.015471037477254868\n",
      "Epoch: 10000\tTotal training loss: 0.04008594527840614\tTest mse: 0.015448184683918953\n",
      "Epoch: 11000\tTotal training loss: 0.040085263550281525\tTest mse: 0.015426335856318474\n",
      "Epoch: 12000\tTotal training loss: 0.04008569195866585\tTest mse: 0.015411006286740303\n",
      "Epoch: 0\tTotal training loss: 0.04010238125920296\tTest mse: 0.015389167703688145\n",
      "Epoch: 1000\tTotal training loss: 0.040080197155475616\tTest mse: 0.015643013641238213\n",
      "Epoch: 2000\tTotal training loss: 0.040079694241285324\tTest mse: 0.015645500272512436\n",
      "Epoch: 3000\tTotal training loss: 0.04007939621806145\tTest mse: 0.015652913600206375\n",
      "Epoch: 4000\tTotal training loss: 0.040079228579998016\tTest mse: 0.015666300430893898\n",
      "Epoch: 5000\tTotal training loss: 0.04007912799715996\tTest mse: 0.015673819929361343\n",
      "Epoch: 6000\tTotal training loss: 0.04007907584309578\tTest mse: 0.015684422105550766\n",
      "Epoch: 7000\tTotal training loss: 0.04007904231548309\tTest mse: 0.015697741881012917\n",
      "Epoch: 8000\tTotal training loss: 0.0400790274143219\tTest mse: 0.01571253128349781\n",
      "Epoch: 9000\tTotal training loss: 0.0400790274143219\tTest mse: 0.015708215534687042\n",
      "Epoch: 10000\tTotal training loss: 0.0400790274143219\tTest mse: 0.01570451632142067\n",
      "error_expr_sorted [(0.01570451632142067, 0.9990841150283813, 1.00059*x_1**3 + 0.926257*x_1**2 + 0.994041*x_1)]\n",
      "Constructing BFGS loss...\n",
      "Final expression:  0.996149239910694*x_1**3 + 0.930238448185546*x_1**2 + 0.999383366971279*x_1\n",
      "Test R2:  0.9990710245460311\n",
      "Test error:  0.0008835680506204533\n",
      "Relative error:  0.1650986328693471\n",
      "Reward:  0.9991172119526935\n",
      "\n",
      "\n",
      "~ Early Stopping Met ~\n",
      "Best expression:  0.996149239910694*x_1**3 + 0.930238448185546*x_1**2 + 0.999383366971279*x_1\n",
      "Best reward:      0.9991172119526935\n",
      "mse error:       0.0008835680506204533\n",
      "Relative error:   0.1650986328693471\n",
      "Expression:  0.996149239910694*x_1**3 + 0.930238448185546*x_1**2 + 0.999383366971279*x_1\n",
      "R2:  0.9990710245460311\n",
      "Error:  0.0008835680506204533\n",
      "Relative Error:  0.1650986328693471\n",
      "log(1 + MSE):  0.0008831779341498579\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Example 1: Input Ground Truth Expression, auto-generate data and extract expression\n",
    "   to use gpu should create a new environment to install paddlepaddle-gpu\n",
    "\"\"\"\n",
    "from DySymNetPP import SymbolicRegression\n",
    "from DySymNetPP.scripts.params import Params\n",
    "from DySymNetPP.scripts.functions import *\n",
    "import numpy as np\n",
    "# Define the hyperparameters for the symbolic neural network\n",
    "config = Params()\n",
    "# Define the functions available for symbolic regression\n",
    "funcs = [Identity(),\n",
    "         Square(),\n",
    "         Pow(3),\n",
    "         Sub(),\n",
    "         Plus(),\n",
    "         Product(),\n",
    "         Div()]\n",
    "config.funcs_avail = funcs\n",
    "config.use_gpu = True\n",
    "config.NOISE = 0.05\n",
    "\n",
    "SR = SymbolicRegression.SymboliRegression(config=config, func=\"x_1**3 + x_1**2 + x_1\", func_name=\"Nguyen-1\")\n",
    "eq, R2, error, relative_error = SR.solve_environment()\n",
    "\n",
    "print('Expression: ', eq)\n",
    "print('R2: ', R2)\n",
    "print('Error: ', error)\n",
    "print('Relative Error: ', relative_error)\n",
    "print('log(1 + MSE): ', np.log(1+error))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
